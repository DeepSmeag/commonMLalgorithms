{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "\n",
    "Now this is the big topic of the last few decades. Since the 2010s, neural networks have become the most popular method applied in the field of ML. Why is that? We'll talk a bit about the history of neural networks, what they needed and lacked and what changed.\n",
    "\n",
    "#### History\n",
    "\n",
    "Neural networks were first introduced in the 1940s by Warren McCulloch and Walter Pitts. They proposed a simple electrical circuit that could be used to model the behavior of neurons. Later, the idea that neurons' pathways strengthen over repeated use, especially between neurons that fire together (Donald Hebb). That was the beginning of mapping the brain to a computer.  \n",
    "\n",
    "The start of the neural networks algorithm came with Frank Rosenblatt in 1957. He proposed the perceptron, a simple neural network that could be used to classify linearly separable data. The perceptron was a single layer neural network with a single output neuron. The input was processed as a weighted sum; then a threshold was applied, and the output was either 0 or 1.\n",
    "\n",
    "![An image of the perceptron; it has an input layer, does a weighted sum over the numbers and applies a threshold to determine the output as 1 or 0](https://miro.medium.com/max/1400/1*ofVdu6L3BDbHyt1Ro8w07Q.png)  \n",
    "Via [Towards Data Science](https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Frosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a&psig=AOvVaw3D5MgtSVmqF_YPMurYUCxv&ust=1674493057232000&source=images&cd=vfe&ved=0CA8QjRxqFwoTCIj4pfDS2_wCFQAAAAAdAAAAABAg)  \n",
    "\n",
    "The goal of the perceptron was to \"learn\" the weights of the linear combination, in order to minimize the difference between predicted output and desired output. Unfortunately, the perceptron could only learn linearly separable data, due to the linear processing applied. Another problem came with the scale of the model itself. By adding multiple layers, the model could learn more complex functions, and finally separate non-linear data. The problem, however, was the sheer complexity of the training task. At the time, the hardware and the methods applied were indicative of the fact that neural networks would lead to nowhere. That's how the first \"AI winter\" came to be, as AI research was abandoned for a while.  \n",
    "\n",
    "In the 1980s, the next efforts to bring AI forward began. Backpropagation had been implemented in the late 1960s, but it was only in the 1980s that people thought to use it for neural networks (we will discuss backpropagation in a future section). By this time, the so-called \"Expert system\" algorithm was adopted by many companies, relying on sets of rules to solve problems and make decisions. Around 1990, the fall of export systems brought a 2nd AI winter, although a shorter one. The developments of multi-layer perceptrons powered by the backpropagation algorithm kept going. The problem was with hardware capabilities. These \"slow learners\" relied on many, many iterations to reach good solutions. Computers needed to go faster. As such, progress was slow, dependent on the speed of the processors.  \n",
    "\n",
    "In the 2000s, the \"AI summer\" came. The development of GPUs (graphics processing units) allowed for a huge increase in the speed of neural networks. The first GPU was released in 1999, and by 2006, the first GPU-based neural network was developed. GPUs' role is to compute a lot of math at the same time. As it so happens, this is exactly what networks need. As such, the speed of the training process increased by a considerable order of magnitude. The development of the internet and the spreading availability of datasets (data) allowed for the development of deep learning, which is the name given to neural networks with many layers (depends on each person, but generally networks with more than 3 hidden layers are considered deep).  \n",
    "In the 2010s, GPUs leaped in performance, cementing the research of neural networks at the front of the AI industry. With time, specialized hardware was introduced, such as TPUs (Tensor Processing Units) and (lately) ML accelerators on mobile devices.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Now that we know a bit about how neural networks came to be, let's talk about what they are and how they work. We've mentioned them before, so now we'll go into detail.  \n",
    "\n",
    "A neural network is model that tries to mimic the human brain. Instead of actual neurons, we're using numbers and functions to achieve results. Each cell in a network is a **neuron**. The connections represent the pathways between neurons. In ML, they are called **weights** (mathematically), or **edges** (graphically, coming from the discipline of graphs). The neurons are organized in layers (input, hidden, output), and we apply **activation functions** after each layer's values have been computed. These activation functions are used to introduce non-linearity into the network (we'll see in more detail as we implement a network).  \n",
    "\n",
    "![A simple network with 3 input neurons connected to a hidden layer with 2 neurons and an output layer with 2 neurons. Each layer's neurons are fully connected to the next batch](../assets/simpleNN.png)  \n",
    "\n",
    "Let's disect this image, shall we?  \n",
    "##### Input layer\n",
    "We have neurons $n_1, n_2, n_3$ in the input layer. These receive the input data. The features we've been using so far are these neurons' values. Quite simple and direct so far. We now have to use and combine them somehow to reach useful information. The weights are the connections between neurons of consecutive layers. These denote the \"importance\" of any given neuron's value.\n",
    "\n",
    "##### Hidden layer\n",
    "These are the intermediary layers that do the bulk of the processing. Let's jump to an example. Using the diagram above, the 2 neurons in the hidden layer would receive the following values:\n",
    "$$\n",
    "\\begin{align}\n",
    "n_{1, hidden} &= w_{1, 1} \\cdot n_1 + w_{2, 1} \\cdot n_2 + w_{3, 1} \\cdot n_3 \\\\\n",
    "n_{2, hidden} &= w_{1, 2} \\cdot n_1 + w_{2, 2} \\cdot n_2 + w_{3, 2} \\cdot n_3\n",
    "\\end{align}\n",
    "$$\n",
    "I haven't numbered them, since it won't be that important. What matters is that we observe the following fact: what we're doing is essentially a linear combination of the input values. Remember when we discussed linear regression? We are following the same steps here. But something is missing: the bias term. Yes, we need it, as we've needed it before. Just as a linear function would not be able to fit to data that does not go through the origin, a neural network cannot fit to certain types of data without the bias term. In terms of how neural networks cover this need, we could have an additional neuron on each layer with a constant value of 1. The weight associated with that neuron would be the bias term.\n",
    "\n",
    "##### Output layer\n",
    "I have drawn 2 neurons in the output layer. It's important to introduce this fact early on: neural networks can produce any number of outputs, so it's a matter of the problem at hand and the result we want to achieve. After the computing has been done, these neurons will hold the final result: the output of the neural network.\n",
    "\n",
    "### Warning: activation functions\n",
    "I would like you to consider the following issue: as we've seen, going from one layer to the next involves computing a linear combination of the values in the previous layer. Apply this any number of times, but in the end what you get is still a linear combination. We know for a fact that a linear combination outputs a line, no matter how many times we apply it. This is problematic. We need to introduce non-linearity into the network, in order to help it fit data in high-dimensional space that is not linearly separable. This is why **activation functions** exist. They are applied after each layer's values have been computed.  \n",
    "\n",
    "There are lots of activation functions out there, and we could in theory use anything, but the purpose is to introduce non-linearity with the cheapest computational cost possible. Here are the most common ones:\n",
    "- Sigmoid  \n",
    "![sigmoid](https://miro.medium.com/max/640/1*Xu7B5y9gp0iL5ooBj7LtWw.webp)  \n",
    "Via [Towards Data Science](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)\n",
    "- Tanh  \n",
    "![tanh](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_4.23.22_PM_dcuMBJl.png)  \n",
    "Via [Papers with code](https://paperswithcode.com/method/tanh-activation)\n",
    "- ReLU  \n",
    "![relu](https://www.nomidl.com/wp-content/uploads/2022/04/image-10.png)  \n",
    "Via [Nomidl](https://www.nomidl.com/deep-learning/what-is-relu-and-sigmoid-activation-function/)\n",
    "- Leaky ReLU  \n",
    "![leaky relu](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_3.09.45_PM.png)  \n",
    "Via [Papers with code](https://paperswithcode.com/method/leaky-relu)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anything that isn't a simple $f(x) = x$ will do. But we have to be careful about the computation involved. This is why the industry turned to ReLU, for it's simplicity and speed. We've talked about gradient descent before. Imagine calculating the derivative of Tanh vs ReLU. The former is a lot more complex, and the latter is a simple $f(x) = 1$ if $x > 0$, and $f(x) = 0$ otherwise. This is why ReLU is so popular. It really might be the most popular out there. We'll talk about the *vanishing gradient* problem after discussing the training of neural networks, and we'll see why sigmoid and tanh are not used as much.  \n",
    "\n",
    "### Some more details\n",
    "\n",
    "There are a few things one should know about neural networks, right from the beginning. They can be arranged in a multitude of ways, which is why lots of architectures exist. We call nets in which all neurons are connected as *fully connected*. We'll see that there are problems that benefit from designing a network with a different topologies.  \n",
    "The more layers & neurons we have, the more complex the data we can fit. The great thing about networks is that they really excel at finding patterns in highly-dimensional, highly-complex data. They don't require human intervention all that much. If we see signs of high bias (underfitting), we can simply add more neurons (existing or new layers). This increases complexity. The problem then is to gather enough data to be able to train these. They are prone to overfitting without a lot of data, since they are so great at fitting despite complexity.  \n",
    "\n",
    "One thing to keep in mind: the parameters we train with NNs are the *weights*. Neurons are simply information-holding cells. Activation functions are static procedures we apply to values. There is nothing we should be modifying except for weights. Now, consider the number of the weights. They are parameters, so for each one we add, we increase the required compute power. In the diagram above, we have 10 weights. That's 10 parameters to tweak. Add the bias terms (in practice, you really should), and we get another 4, so that's 14 total. Things get really complicated really fast."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: how do NNs work?\n",
    "\n",
    "We get our features/values into the input layer.  \n",
    "Those values are part of linear combinations, which go to the next layer.  \n",
    "Each neuron in that next layer applies an \"activation function\" (a non-linear function), which updates the values.  \n",
    "Those are the final values of that layer.  \n",
    "The next step is to repeat the process for the next layer, until we reach the output layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training: How do we find the optimal parameters?\n",
    "\n",
    "Starting with the basics, neural networks are a supervised algorithm. This means we know the output. This also means we can define error/loss functions. These tell us how far from the real answer we are.  \n",
    "The next step is to define a method that brings us closer to the right values for the parameters. We could guess and check. Imagine how slow that would go, based on the fact that a simple network like the one above should have 14 parameters to tweak. So then we use math. We turn to *gradient descent* and *backpropagation*.  \n",
    "\n",
    "## Gradient descent\n",
    "\n",
    "Gradient descent is a method that allows us to find the minimum of a function, by continuously updating our parameters to go towards the direction of the steepest descent. We've already seen it in action, so the new element is backpropagation. Let's check the simple network again:  \n",
    "![The same network from before](../assets/simpleNN.png)  \n",
    "Our parameters are the weights between the neurons. Our error is after the output layer. We can reach the weights just before the output layer, since we know the steps that were applied to calculate our final output.  \n",
    "We will make some notations:  \n",
    "$g_1(x) = w_1x + b_1$  \n",
    "$g_2(x) = w_2x + b_2$  \n",
    "Let's call our output neurons $o_1$ and $o_2$. Their values would be:  \n",
    "$o_1 = f(g_1(x))$  \n",
    "$o_2 = f(g_2(x))$  \n",
    "\n",
    "Where we would have $f$ as the activation function, $w_i$ as the corresponding set of weights, $b_i$ as the bias terms, and $x$ as the values of the previous layer of neurons.  \n",
    "\n",
    "By applying our error function to these $o_i$ values, we get the error. In order to reach the error with respect to the weights, we need to get through some steps. We apply *the chain rule*:  \n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_1} = \\frac{\\partial E}{\\partial o_1} \\frac{\\partial o_1}{\\partial w_1} = \\frac{\\partial E}{\\partial o_1} \\frac{\\partial o_1}{\\partial g_1} \\frac{\\partial g_1}{\\partial w_1}\n",
    "\\\\\n",
    "\\text{} \\\\\n",
    "\\\\\n",
    "\\frac{\\partial E}{\\partial w_2} = \\frac{\\partial E}{\\partial o_2} \\frac{\\partial o_2}{\\partial w_2} = \\frac{\\partial E}{\\partial o_2} \\frac{\\partial o_2}{\\partial g_2} \\frac{\\partial g_2}{\\partial w_2}\n",
    "$$  \n",
    "\n",
    "As you can see, terms can be nullified if we're looking diagonally."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A bit of math\n",
    "\n",
    "We call it the chain rule because it helps us get through the chain of operations that led to the final output. We can calculate how much each part of the whole equation contributed to the final error.  \n",
    "In order to properly calculate some values (or get some equations), we have to choose our functions. Let's suppose our NN classifies some data. Both our outputs would be a 1 or a 0. For the error function, we'll choose the squared error, and the activation function will be ReLU.  \n",
    "\n",
    "Let's list them:\n",
    "$$\n",
    "\\begin{align}\n",
    "E &= \\frac{1}{2}(o_1 - y_1)^2 + \\frac{1}{2}(o_2 - y_2)^2 \\\\\n",
    "o_1 &= f(w_1x + b_1) \\\\\n",
    "o_2 &= f(w_2x + b_2) \\\\\n",
    "f(x) &= \\begin{cases} 0 & x < 0 \\\\ x & x \\geq 0 \\end{cases} \\\\\n",
    "o_i &= \\begin{cases} 0 & w_ix + b_i < 0 \\\\ w_ix + b_i & w_ix + b_i \\geq 0 \\end{cases} \\\\\n",
    "\\end{align}\n",
    "$$  \n",
    "\n",
    "So then, let's consider our first weight, $w_1$. Here are our derivatives:  \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial o_1} &= o_1 - y_1 \\\\\n",
    "\\frac{\\partial o_1}{\\partial w_1} &= \\begin{cases} 0 & w_1x + b_1 \\leq 0 \\\\ x & w_1x + b_i > 0 \\end{cases} \\\\\n",
    "\n",
    "\\frac{\\partial E}{\\partial w_1} &= (o_1 - y_1) * \\frac{\\partial o_1}{\\partial w_1} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\text{Note: Derivative of ReLU is undefined in 0, but we can set it to 0, since that would be the value of the function in that point.} \\\\\n",
    "$$\n",
    "\n",
    "\n",
    "Of course, in these formulas, x would be a vector, which is not the right way to go about this. We considered $w_1$ and $w_2$ as sets of weights. In practice, we take each constituent weight and calculate the error with respect to it. The $x$ would be the corresponding output of the previous neuron. Note: we also calculate the error with respect to the bias terms. They don't get the $x$ in the formula.  \n",
    "\n",
    "Let's suppose we did our math correctly, we got the error with respect to the weights just before the output layer. As we know, GD tells us to now update their values by subtracting the error times a learning rate. We know the drill by know. But...  \n",
    "\n",
    "What about the previous layers? Well, it turns out we do essentially the same steps, but more of them. This is where backpropagation comes in."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "Backpropagation is the process of propagating the error of the weights in the previous layers (back + propagation, makes sense right?). We do this by applying the chain rule. We start with the output layer, and we go backwards. We calculate the error of the weights just before the output layer, then we calculate the error of the weights just before that layer, and so on. In short, we apply gradient descent per layer going from output to input. Of course, the input layer has no weights to update, so we stop there.  \n",
    "\n",
    "As such, we call the prediction part of the network the *forward pass* (forward propagation), and the training part the *backward pass* (backpropagation).  \n",
    "\n",
    "As an example, the $w_11$ weight after the input layer would have the following error:  \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial w_{11}} &= \\frac{\\partial E}{\\partial o_1} \\frac{\\partial o_1}{\\partial w_{11}} + \\frac{\\partial E}{\\partial o_2} \\frac{\\partial o_2}{\\partial w_{11}} \\\\\n",
    "&= (o_1 - y_1) \\frac{\\partial o_1}{\\partial w_{11}} + (o_2 - y_2) \\frac{\\partial o_2}{\\partial w_{11}} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $o_i$ is the output of a neuron in the output layer.  \n",
    "We're missing the derivatives of $o_i$ with respect to $w_{11}$. But $w_{11}$ is not directly linked to any of the outputs. How do we calculate this, then?  \n",
    "We're going to go in-depth, since this seems to be the only way to make sure that everything makes sense."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop, step by step\n",
    "\n",
    "In order to do this properly, we'll need a more detailed diagram of our network. This time, each notation is important so we can keep track of what's happening.  \n",
    "\n",
    "![Diagram of neural network with notations in order. Input has 3 neurons. There is 1 hidden layer with 2 neurons. Output has 2 neurons. Bias is displayed for input and hidden layer. Notations for weights are w with i and j indices, where i is the index of the neuron in the 1st layer, and j is the index of the neuron in the 2nd layer. Weights also have a superscript, which is the index of the layer they go into. We've considered the hidden layer as layer 1. Bias is denoted by a b with an index and a superscript with the layer. Notation before activation function of a layer is H index i and superscript with layer. The hidden layer is denoted by Z and index after activation function. The output layer just has the notation O after the activation function.](../assets/simpleNNdetailed.png)  \n",
    "This one diagram is going to help us make sense of everything that is going on. I would recommend drawing it on a piece of paper before going forward.  \n",
    "Let's first go through what each notation means.  \n",
    "\n",
    "Input: we have 3 input variables, $x_1$, $x_2$ and $x_3$. This time, we have drawn the bias terms. We consider a fictitious input that has a constant value of 1. If we do the calculations normally, we just multiply the bias by 1, which leaves the bias term unchanged. This helps us scale computation later on.  \n",
    "The indexes correspond to the neurons in each pair of layers. The superscript is the index of the layer that the weight goes into (if we had multiple hidden layers, we would have superscripted the z's as well). We consider the hidden layer as layer 1.  With these notations in mind, you should be able to trace out the forward pass through the network. We're going to go through the backward pass together.  \n",
    "\n",
    "##### Note: Any kind of number a variable influences, that number can be partially derived with respect to that variable. No matter how many things were added on top, we can always use the chain rule to get back to that one variable. You will see this in action as we go.\n",
    "\n",
    "A reminder on some notations and formulas we use:\n",
    "$$\n",
    "\\begin{align}\n",
    "h_1^{(1)} &= \\sum_{i=1}^3 w_{i1}^{(1)}x_i + b_1^{(1)} \\\\\n",
    "z_1 &= f(h_1^{(1)}) \\\\\n",
    "f(x) &= \\begin{cases} 0 & x < 0 \\\\ x & x \\geq 0 \\end{cases} \\\\\n",
    "h_1^{(2)} &= \\sum_{i=1}^2 w_{i1}^{(2)}z_i + b_1^{(2)} \\\\\n",
    "o_1 &= f(h_1^{(2)}) \\\\\n",
    "E &= \\frac{1}{2} \\sum_{i=1}^2 (o_i - y_i)^2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\text{$y_i$ is the ground truth for the i-th output neuron.} \\\\\n",
    "$$  \n",
    "The rest of the terms are calculated similarly to these.  \n",
    "As we go through the process, I highly recommend writing everything down. It helps with understanding."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the error of the output layer\n",
    "\n",
    "$$\n",
    "E = \\frac{1}{2} \\sum_{i=1}^2 (o_i - y_i)^2 \\\\\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial o_1} &= o_1 - y_1 \\\\\n",
    "\\frac{\\partial E}{\\partial o_2} &= o_2 - y_2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Going further:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial h_1^{(2)}} &= \\frac{\\partial E}{\\partial o_1} \\frac{\\partial o_1}{\\partial h_1^{(2)}} \\\\\n",
    "\\frac{\\partial o_1}{\\partial h_1^{(2)}} &= \\begin{cases} 1 & h_1^{(2)} > 0 \\\\ 0 & h_1^{(2)} \\leq 0 \\end{cases} \\\\\n",
    "\\frac{\\partial E}{\\partial h_1^{(2)}} &= (o_1 - y_1) ,&  h_1^{(2)} > 0\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "We're supposing that these branches go in the \"$> 0$\" direction, since otherwise it's not that useful. This helps with making things clearer.\n",
    "$\\frac{\\partial E}{\\partial h_2^{(2)}}$ is analogous.  \n",
    "\n",
    "Finally, going for the weights (the part that really interests us, since they are the variables we can actually change):\n",
    "$$\n",
    "\\begin{align}\n",
    "h_1^{(2)} &= \\sum_{i=1}^2 w_{i1}^{(2)}z_i + b_1^{(2)} \\\\\n",
    "\\frac{\\partial E}{\\partial w_{11}^{(2)}} &= \\frac{\\partial E}{\\partial h_1^{(2)}} \\frac{\\partial h_1^{(2)}}{\\partial w_{11}^{(2)}} \\\\\n",
    "\\frac{\\partial h_1^{(2)}}{\\partial w_{11}^{(2)}} &= z_1 \\\\\n",
    "\\frac{\\partial E}{\\partial w_{11}^{(2)}} &= (o_1 - y_1) z_1 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Similarly...\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial h_1^{(2)}}{\\partial b_1^{(2)}} &= 1 \\\\\n",
    "\\frac{\\partial E}{\\partial b_1^{(2)}} &= \\frac{\\partial E}{\\partial h_1^{(2)}} \\frac{\\partial h_1^{(2)}}{\\partial b_1^{(2)}} \\\\\n",
    "&= (o_1 - y_1) \\\\\n",
    "\\end{align}\n",
    "$$  \n",
    "\n",
    "So we know the derivatives of the error with respect to the weights and biases of the output layer. The formulas are similar for the others. We can now update their values, as per GD rules:\n",
    "$$\n",
    "\\begin{align}\n",
    "w_{11}^{(2)} &= w_{11}^{(2)} - \\alpha \\frac{\\partial E}{\\partial w_{11}^{(2)}} \\\\  \n",
    "b_1^{(2)} &= b_1^{(2)} - \\alpha \\frac{\\partial E}{\\partial b_1^{(2)}} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "##### In practice, we don't update the values right away, since these would be used further down the line. We could either store some values in temporary variables, or we could update them after calculating what's needed for the next layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the error of the hidden layer\n",
    "\n",
    "The first step is to calculate $\\frac{\\partial E}{\\partial z_1}$ and $\\frac{\\partial E}{\\partial z_2}$.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial z_1} &= \\frac{\\partial E}{\\partial h_1^{(2)}} \\frac{\\partial h_1^{(2)}}{\\partial z_1} \\\\\n",
    "\\frac{\\partial h_1^{(2)}}{\\partial z_1} &= w_{11}^{(2)} \\\\\n",
    "\\frac{\\partial E}{\\partial z_1} &= (o_1 - y_1) w_{11}^{(2)} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### !!\n",
    "But here is the thing: $z_1$ influences the output in both $o_1$ and $o_2$. So we need to add the influence of $z_1$ on $o_2$ as well. The formula above is incomplete, here is the correct version:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial z_1} &= \\frac{\\partial E}{\\partial h_1^{(2)}} \\frac{\\partial h_1^{(2)}}{\\partial z_1} + \\frac{\\partial E}{\\partial h_2^{(2)}} \\frac{\\partial h_2^{(2)}}{\\partial z_1}\\\\\n",
    "\\frac{\\partial h_2^{(2)}}{\\partial z_1} &= w_{12}^{(2)} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial z_1} &= (o_1 - y_1) w_{11}^{(2)} + (o_2 - y_2) w_{12}^{(2)} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "The same goes for $z_2$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial h_1^{(1)}} &= \\frac{\\partial E}{\\partial z_1} \\frac{\\partial z_1}{\\partial h_1^{(1)}} \\\\\n",
    "\\frac{\\partial z_1}{\\partial h_1^{(1)}} &= \\begin{cases} 1 & h_1^{(1)} > 0 \\\\ 0 & h_1^{(1)} \\leq 0 \\end{cases} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "(By now, you probably see why ReLU is so popular. No extra computation required).  \n",
    "\n",
    "Now we can move on to weights.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h_1^{(1)} &= \\sum_{i=1}^2 w_{i1}^{(1)}x_i + b_1^{(1)} \\\\\n",
    "\\frac{\\partial E}{\\partial w_{11}^{(1)}} &= \\frac{\\partial E}{\\partial h_1^{(1)}} \\frac{\\partial h_1^{(1)}}{\\partial w_{11}^{(1)}} \\\\\n",
    "\\frac{\\partial h_1^{(1)}}{\\partial w_{11}^{(1)}} &= x_1 \\\\\n",
    "\\frac{\\partial E}{\\partial w_{11}^{(1)}} &= ((o_1 - y_1) w_{11}^{(2)} + (o_2 - y_2) w_{12}^{(2)}) x_1 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Similarly, for the bias:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial h_1^{(1)}}{\\partial b_1^{(1)}} &= 1 \\\\\n",
    "\\frac{\\partial E}{\\partial b_1^{(1)}} &= \\frac{\\partial E}{\\partial h_1^{(1)}} \\frac{\\partial h_1^{(1)}}{\\partial b_1^{(1)}} \\\\\n",
    "&= ((o_1 - y_1) w_{11}^{(2)} + (o_2 - y_2) w_{12}^{(2)}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And that's it! The rest of the weights are analogous, so no new formulas are needed. If you understand why things work out the way they do, you can easily extend this process to any number of hidden layers. But we won't stop here. As you can see, the formulas require some information to be calculated, some to be stored, and some to be updated. As we scale to bigger networks, doing this by hand is too tedious. That's why we use matrices, vectors, and their associated operations to make things scale easily. We also see some terms being reused. Once we compute the derivatives with respect to the $h_i^{(l)}$'s of a layer, we do not have to go back to the output layer. It's like a savepoint in a game. This is why we introduce the following name: \"**local gradients**\".  \n",
    "\n",
    "Here is the idea and notation:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta_1^{(2)} &= \\frac{\\partial E}{\\partial h_1^{(2)}}\\\\\n",
    "\\delta_2^{(2)} &= \\frac{\\partial E}{\\partial h_2^{(2)}}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "These are the local gradients of the output layer. We already know how they are calculated. We use them to obtain the relevant information about derivatives with respect to the weights and biases of the 2nd layer. Once we continue our backpropagation process, we reach the following:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta_1^{(1)} &= \\frac{\\partial E}{\\partial h_1^{(1)}}\\\\\n",
    "\\delta_2^{(1)} &= \\frac{\\partial E}{\\partial h_2^{(1)}}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Again, we know how to calculate these. We just have to use them. As we've mentioned earlier, these values are reused, so we don't have to go back to the output layer. In implementation terms, we store the local gradients of layer $(l)$ before updating the weights of layer $(l+1)$. This is the way we guarantee that we don't use the updated weights to calculate the local gradients of layer $(l)$.  \n",
    "And with these in mind, backpropagation is even easier to understand and implement. We just start from the output layer, calculate our way backwards, and save progress as we go."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "As we know, vectorization allows for faster processing times (in general, due to parallelization). We'll see how dot products and transposes help us vectorize our code. \n",
    "Let's gather our matrices and vectors:\n",
    "\n",
    "$$\n",
    "\\text{Let $X$ be the input vector}\\\\\n",
    "X = \\begin{bmatrix}\n",
    "x_{1} \\\n",
    "x_{2} \\\n",
    "x_{3} \\\n",
    "1 \\\n",
    "\\end{bmatrix} \\\\\n",
    "\\text{Let $W^{(1)}$ be the matrix of weights of the 1st layer}\\\\\n",
    "\\\\\n",
    "W^{(1)} = \\begin{bmatrix}\n",
    "w_{11}^{(1)} & w_{12}^{(1)} \\\\\n",
    "w_{21}^{(1)} & w_{22}^{(1)} \\\\ \n",
    "w_{31}^{(1)} & w_{32}^{(1)} \\\\\n",
    "b_{1}^{(1)} & b_{2}^{(1)} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\text{Let $W^{(2)}$ be the matrix of weights of the 2nd layer}\\\\\n",
    "\\\\\n",
    "W^{(2)} = \\begin{bmatrix}\n",
    "w_{11}^{(2)} & w_{12}^{(2)} \\\\\n",
    "w_{21}^{(2)} & w_{22}^{(2)} \\\\\n",
    "b_{1}^{(2)} & b_{2}^{(2)} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\text{Let $y$ be the vector of outputs}\\\\\n",
    "\\\\\n",
    "y = \\begin{bmatrix}\n",
    "y_{1} \\\n",
    "y_{2} \\\n",
    "\\end{bmatrix} \\\\\n",
    "\\text{Let $O$ be the vector of outputs}\\\\\n",
    "\\\\\n",
    "O = \\begin{bmatrix}\n",
    "o_{1} \\\n",
    "o_{2} \\\n",
    "\\end{bmatrix}\n",
    "$$  \n",
    " \n",
    "As you can see, we've added the bias terms to the weights matrices. To account for this, we add 1's to the input vector and the $Z^{(l)}$ vectors. This is the way we make sure that the bias terms are taken into account.  \n",
    "\n",
    "---\n",
    "\n",
    "$$\n",
    "\\text{$H^{(1)}$ is the vector of outputs of the 1st layer, and $H^{(2)}$ for the 2nd layer}\\\\\n",
    "\\\\\n",
    "H^{(1)} = \\begin{bmatrix}\n",
    "h_{1}^{(1)} \\\n",
    "h_{2}^{(1)} \\\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "Z^{(1)} = \\begin{bmatrix}\n",
    "z_{1}^{(1)} \\\n",
    "z_{2}^{(1)} \\\n",
    "1 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "H^{(2)} = \\begin{bmatrix}\n",
    "h_{1}^{(2)} \\\n",
    "h_{2}^{(2)} \\\n",
    "\\end{bmatrix} \\\\\n",
    "$$\n",
    "\n",
    "As per these notations, we can write the following:\n",
    "$$\n",
    "\\begin{align}\n",
    "H^{(1)} &= W^{(1)'} \\cdot X &\\text{' denotes the transpose}\\\\\n",
    "Z^{(1)} &= f(H^{(1)}) & \\text{where $f$ is the activation function, and we add the 1 after applying it} \\\\\n",
    "H^{(2)} &= W^{(2)'} \\cdot Z^{(1)} \\\\\n",
    "O &= f(H^{(2)}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "It's quite easy to follow these in the forward pass. With these notations, we can do the backpropagation process in a vectorized way.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta_1^{(2)} &= \\frac{\\partial E}{\\partial h_1^{(2)}} \\\\\n",
    "&= (o_1 - y_1) f'(h_1^{(2)}) \\\\\n",
    "\\delta_2^{(2)} &= \\frac{\\partial E}{\\partial h_2^{(2)}} \\\\\n",
    "&= (o_2 - y_2) f'(h_2^{(2)}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Is equivalent to:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta^{(2)} &= \\frac{\\partial E}{\\partial H^{(2)}} \\\\\n",
    "&= (O - Y) * f'(H^{(2)}) & \\text{with dimensions 2x1} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Where $Y$ is the vector of expected outputs, * means element-wise multiplication, and $f'$ is the derivative of the activation function.  \n",
    "\n",
    "The derivatives with respect to the weights and biases of the 2nd layer are:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial W^{(2)}} &= \\frac{\\partial E}{\\partial H^{(2)}}\\frac{\\partial H^{(2)}}{\\partial W^{(2)}} = Z^{(1)} \\cdot \\delta^{(2)'} & \\text{with dimensions 3x2, so we can subtract right away}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "We don't need separate derivatives with respect to the biases, because they are included in the W vector.\n",
    "\n",
    "Going to the 1st layer, we have:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta_1^{(1)} &= \\frac{\\partial E}{\\partial h_1^{(1)}} \\\\\n",
    "&= \\delta_1^{(2)} \\cdot w_{11}^{(2)} + \\delta_2^{(2)} \\cdot w_{12}^{(2)} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Which is equivalent to:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta^{(1)} &= \\frac{\\partial E}{\\partial H^{(1)}} \\\\\n",
    "&= W^{(2)} \\cdot \\delta^{(2)} & \\text{with dimensions 2x1; for this one, we would only consider the weights part of the W vector} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "And the derivatives with respect to the weights and biases of the 1st layer are:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial W^{(1)}} &= \\frac{\\partial E}{\\partial H^{(1)}}\\frac{\\partial H^{(1)}}{\\partial W^{(1)}} = X \\cdot \\delta^{(1)'} & \\text{with dimensions 4x2, so we can subtract right away}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And that's it!  \n",
    "As soon as we've calculated $\\delta^{(2)}$, we can update the weights of the 2nd layer as GD indicates. The weights for the 1st layer are updated at the end, since that is the final layer. If we had multiple layers, we can generalize the process: update the weights of layer $l+1$ after calculating the local gradient of layer $l$.  \n",
    "In case we feel like we have enough memory/storage, we could save the information about the gradients, but it's really not necessary. We'll see how the implementation goes once we get to it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing Gradient  \n",
    "\n",
    "We've been hearing a lot about gradients, and how they are used to update the weights. We've seen that we multiply lots of things together. The problem is with what kinds of numbers we multiply. In theory, we see mostly letters. The notations we use hide the fact that real applications tend to work with really small numbers. As layers are added, the backpropagation algorithm has us multiply lots of small numbers together. There comes a point at which the gradients are so small, they are practically 0. They \"vanish\", which is why we call this the \"*vanishing gradient problem*\".  \n",
    "\n",
    "To counterract this problem, we try to use activation functions that have a larger derivative. Guess what, we've been doing that all along! ReLU is one of the functions that allows us to keep our gradients as untouched as possible, since we generally multiply the numbers by 1, so we do nothing to them.  \n",
    "\n",
    "If you would look at the derivative of the sigmoid function, you would notice (even if by eye) that it is quite small. Numbers aren't necessary here, since we know for sure it's going to be quite smaller than 1. This is why we use ReLU, which has a derivative of 1 for all positive numbers. These choices are very important, since they affect the performance of a neural network with many hidden layers. As a trivia side note, once a network has more than 2 hidden layers, it is called a *deep neural network*.  \n",
    "\n",
    "These days, deep neural networks can have tens, if not hundreds or thousands of layers. That would be the new definition of \"deep\" today."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression vs Classification\n",
    "\n",
    "Up until this point, we've been talking about regression. Of course, neural networks can also classify data. You might be wondering about the modifications we would need to make to the network. Not much, actually. No matter the architecture, purpose, or other details, a neural network is meant to do calculations on numbers and output something we might find useful.  \n",
    "In the case of classification, the modifications we would need have to do with the activation function. Classification (especially when we have multiple output neurons) needs numbers that are between 0 and 1. When there are multiple percentages, they need to add to 1. A function that does this is the *softmax* function. Out of a list of numbers, it normalizez them so that they represent percentages of a whole, so they add to 1. In other word, it turns the outputs into probabilities of the input belonging to a certain class.  \n",
    "\n",
    "Another solution to the classifications problem is the *sigmoid* function, which is a special case of the softmax function. It is used when we have only 2 classes, since it maps any real number to the (0,1) interval. We've mentioned it as an activation function (but not popular due to the vanishing gradient problem), it can also be used as the output function, but it's not the most common choice since the softmax function is more general.  \n",
    "Yet another, more primitive, solution is a hard threshold. We do our calculations, and if the output is above a certain threshold, we classify it as belonging to one class, and if it's below, we classify it as belonging to the other class. This is a very rigid option, and you probably won't ever see it in practice outside teaching examples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "We've discussed the concepts and the math involved, so we should now get on with the implementation. To keep in line with the \"non-linearly separable\" requirement and the multiple output situation, I'll be trying to predict the result of an XOR operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# XOR visualization in 2D\n",
    "# XOR data\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "# Visualize in 2D\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=200, cmap=plt.cm.Spectral)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is no line that can separate the 2 classes. As such, we have a non-linearly separable classification problem. This calls for at least 1 hidden layer, since a single layer of weights would just create a linear regression/classification model.  \n",
    "\n",
    "We will have:\n",
    "* 2 input neurons\n",
    "* 1 hidden layer with X neurons (we can choose X as we like; we would ideally think about the number of features each layer should detect, and that would be the number of neurons in that layer; think about what a feature could be, based on our experience so far; for example, a feature could be \"the presence of a certain pattern\", like a line, a curve, a circle, or a tail in a dog's picture; we will be talking about images and how to deal with them in the future); we will choose X=3 this time and see how it goes\n",
    "* 2 output neurons (we could have only 1 neuron and interpret it's value as the probability of the input belonging to the 1st class, but we will have 2 neurons for the sake of practicing multiple outputs); each one will represent the probability of the input belonging to the i-th class (XOR=0 if $output_0$ is 1, XOR=1 if $output_1$ is 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset\n",
    "# XOR data\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array(\n",
    "    [[1, 0], [0, 1], [0, 1], [1, 0]]\n",
    ")  # we use the labels this way because we have 2 output neurons\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn NN implementation, just to test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "[[0.9964806  0.00323421]\n",
      " [0.00182489 0.99833294]\n",
      " [0.02147105 0.9792537 ]\n",
      " [0.99731864 0.00245544]]\n"
     ]
    }
   ],
   "source": [
    "# scikit\n",
    "# Create the model\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=(3),\n",
    "    activation=\"relu\",\n",
    "    solver=\"sgd\",\n",
    "    learning_rate_init=0.1,\n",
    "    max_iter=1000,\n",
    ")\n",
    "# Train the model\n",
    "model.fit(X, y)\n",
    "# Predict\n",
    "print(model.predict(X))\n",
    "# Predict probabilities\n",
    "print(model.predict_proba(X))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some things to unpack before doing our own implementation.  \n",
    "First of all, the MLPClassifier is a multi-layer perceptron classifier. It is a neural network that can be used for classification. The parameters are as follows:\n",
    "* **hidden_layer_sizes**: the number of neurons in each hidden layer; we can have multiple hidden layers, so we can pass a tuple with the number of neurons in each layer; for example, (3,2) would mean 2 hidden layers, with 3 neurons in the 1st layer and 2 neurons in the 2nd layer\n",
    "* **activation**: the activation function to use; we can choose between \"identity\", \"logistic\", \"tanh\", \"relu\"; we used \"relu\" for this example, and results should be almost perfect, considering the small network; try playing around with the other options (especially \"logistic\" and \"tanh\") and see how the results change; for example, choosing \"logistic\" should give terrible results, in comparison to \"relu\"\n",
    "* **solver**: the solver for weight optimization; we can choose between \"lbfgs\", \"sgd\", \"adam\"; \"sgd\" = stochastic gradient descent, \"adam\" = adaptive moment estimation (a better version of sgd, in terms of adaptivity); we used sgd here, since we've talked about batch and stochastic gradient descent before; adam is more used in real applications, since it's more efficient\n",
    "* **learning_rate_init**: we know about learning rate, but the \"init\" part is new; it implies that the learning rate changes over time; there is another optional argument [learning_rate], which lets us choose if we want to use a constant learning rate, or a learning rate that changes over time; we used \"constant\" here (as the default value so it's not even in the code), but you can try playing around with it and see how it affects the results; it only works with \"sgd\"\n",
    "* **max_iter**: the maximum number of iterations the solver will run; there are mechanisms in place to stop the training early.  \n",
    "\n",
    "For more information, check out the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).  \n",
    "\n",
    "!!!! Always make decisions based on your end goal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our own implementation, but first...\n",
    "\n",
    "Yes, we're still not totally ready. There is one more concept we should discuss.  \n",
    "\n",
    "##### Breaking the simmetry. How to initialize weights.\n",
    "\n",
    "Simple terms: simmetry = the same in both directions.  \n",
    "Especially when using ReLU, there is a problem that can occur. Suppose we have 2 adjacent hidden layers at some point in the network.  \n",
    "\n",
    "Initializing their weights with 0 is an *easy no-go*. Every layer's value will be 0, and the activation function will be 0 for all values going forward from that point. So no layer can have a full set of weights equal to 0 at any point. There may be cases where training / optimization decides that some weights are not needed, but that's for the training process to decide.  \n",
    "\n",
    "The next problem: having negative weights. If, at any point, the values of neurons become negative (including 0), their derivatives, by definition (of ReLU), become 0. That means (remember the local gradients, the chain rule and the training formulas) that the weights of that previous layer will not be updated, similarly to what the vanishing gradient problem does. The conclusion is that weights should gravitate towards positive values, at least in the beginning. The training process will decide afterwards.\n",
    "  \n",
    "Finally: if, at any point, the weights become so similar (or downright equal) that the neurons they feed into have the same values, we essentially get multiple layers that do the same thing. This is the problem of **simmetry**. So avoid initializing with constants (or at the very least, the *same* constants).\n",
    "\n",
    "**The solution**: initialize weights with random values, but don't let them get too big. Big values can cause the network (since formulas lead us to multiplying by those big values at some point) to keep jumping around and diverge. Make sure the bias terms also follow the same rules, since a big negative value (mathematically small, actually) in the bias can essentially make the neuron \"**dead**\" (since the activation function will be 0 for all values - this is called the *dead neuron problem*).  \n",
    "\n",
    "*Final, final solution*: just randomly picking numbers for the weights isn't enough, since we still have a chance to run into the problems we've mentioned above. In practice, we pick our random numbers from certain distributions, so we're sure that our weights have the best chances of avoiding simmetry, diverging and dead neurons. As such, the normal distribution with a mean of 0 and a standard deviation of 1 is the most common choice. Of course, with negative values we still have the chance of getting that dead neuron problem, but getting 1 dead neuron vs all of them makes a big difference. In practice, we would interpret this as a feature that is unimportant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the seed, we should get the same matrices every time we run the code for the first time\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28758257 0.71241743]\n",
      " [0.70680128 0.29319872]\n",
      " [0.36666706 0.63333294]\n",
      " [0.83582949 0.16417051]]\n",
      "[[1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Ready to write code now\n",
    "# We have the data X and y\n",
    "# Building each layer\n",
    "Xb = np.hstack(\n",
    "    [X, np.ones((X.shape[0], 1))]\n",
    ")  # add the bias, now an input set looks like [value1, value2, 1]\n",
    "W1 = np.random.randn(3, 3)  # 2 inputs + 1 bias, 3 neurons in the hidden layer\n",
    "H1 = Xb.dot(W1)  # 1st hidden layer values; size:\n",
    "A1 = np.maximum(H1, 0)  # ReLU activation\n",
    "Z1 = np.hstack(\n",
    "    [A1, np.ones((A1.shape[0], 1))]\n",
    ")  # add the bias, now an input set looks like [value1, value2, 1]\n",
    "W2 = np.random.randn(4, 2)  # 3 inputs + 1 bias, 2 neurons in the output layer\n",
    "H2 = Z1.dot(W2)  # 2nd hidden layer values\n",
    "Z2 = np.maximum(H2, 0)  # ReLU activation; this is the output layer\n",
    "# Softmax for final probabilities\n",
    "exp_scores = np.exp(Z2)\n",
    "probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "# Predictions\n",
    "# predictions = np.argmax(probs, axis=1)\n",
    "print(probs)\n",
    "print(y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a manual forward propagation, we would better benefit from using functions for these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for forward propagation\n",
    "# Returns decision vector for each input set\n",
    "def forward(X, W1, W2):\n",
    "    Xb = np.hstack(\n",
    "        [X, np.ones((X.shape[0], 1))]\n",
    "    )  # add the bias, now an input set looks like [value1, value2, 1]\n",
    "    H1 = Xb.dot(W1)  # 1st hidden layer values\n",
    "    A1 = np.maximum(H1, 0)  # ReLU activation\n",
    "    Z1 = np.hstack(\n",
    "        [A1, np.ones((A1.shape[0], 1))]\n",
    "    )  # add the bias, now an input set looks like [value1, value2, 1]\n",
    "    H2 = Z1.dot(W2)  # 2nd hidden layer values\n",
    "    # Softmax for final probabilities\n",
    "    exp_scores = np.exp(H2)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return Z1, probs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty much random, since that's all we've done for now. We're now getting into the training process. We will be doing *batch gradient descent* (meaning we update after each epoch with the gradients of the whole training set, not after each sample; we can do this since the training set is so small; in other situations mini-batch / stochastic GD is preferred). Modifying the code for the sample should be easy enough as a separate exercise.  \n",
    "\n",
    "Note: we cannot apply the ReLU activation function in the output layer, since it does a few bad things for us:  \n",
    "- negative values get squashed to 0 (which is a good thing since we want probabilities, aka positive values; but consider regressions where ReLU essentially evaporates results below 0); one more thing - all values below 0 get mapped to the same value, so they get the same importance; when evaluating any kind of score, we want to differentiate values; ReLU equalizes them, so avoid it in the output layer\n",
    "- it doesn't squish positive values to 1 (so it doesn't guarantee probabilities; we can get weird results)  \n",
    "So we used the softmax function to map values to probabilities, and then we use GD normally with our cost function to optimize the weights.\n",
    "\n",
    "!!!! Remember: with GD, we calculate the gradient of the cost function with respect to the weights, but the formulas we presented were applied to only 1 training instance at a time; in practice, we want to leverage parallelism, so we use the same calculations, but by using the dot product they can be applied to the whole training set at once. We can then average the results by dividing by the number of training instances, or use the learning rate to scale the results (or both; in the end, the difference at this point is in scale only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 3.109240\n",
      "Loss after iteration 1000: 0.262776\n",
      "Loss after iteration 2000: 0.055071\n",
      "Loss after iteration 3000: 0.027412\n",
      "Loss after iteration 4000: 0.017533\n",
      "Loss after iteration 5000: 0.012641\n",
      "Loss after iteration 6000: 0.009770\n",
      "Loss after iteration 7000: 0.007901\n",
      "Loss after iteration 8000: 0.006598\n",
      "Loss after iteration 9000: 0.005641\n"
     ]
    }
   ],
   "source": [
    "# define function for training\n",
    "# parameters: learning_rate, number of iterations\n",
    "# we're considering the structure as fixed, we won't be doing an algorithm for any kind of NN size and structure\n",
    "def train(X, y, W1, W2, learning_rate=0.01, num_iterations=10000, print_loss=False):\n",
    "    Xb = np.hstack(\n",
    "        [X, np.ones((X.shape[0], 1))]\n",
    "    )  # add the bias, now an input set looks like [value1, value2, 1]\n",
    "    loss = []\n",
    "    for iter in range(num_iterations):\n",
    "        # Forward propagation\n",
    "        Z1, probs = forward(X, W1, W2)\n",
    "        lossTemp = -np.sum(y * np.log(probs))\n",
    "        loss.append(lossTemp)\n",
    "\n",
    "        dW1 = np.zeros_like(W1)\n",
    "        dW2 = np.zeros_like(W2)\n",
    "        # Backpropagation\n",
    "    \n",
    "        cost = probs - y\n",
    "        # Output layer\n",
    "        dZ2 = np.mat(cost)  # 1x2\n",
    "        Z1t = np.mat(Z1)  # 1x4\n",
    "        dW2 += np.dot(Z1t.T, dZ2)  # 4x2\n",
    "        # Hidden layer\n",
    "        dZ1 = np.dot(dZ2, W2.T) \n",
    "        dZ1 = np.multiply(dZ1, (Z1t > 0))\n",
    "        dZ1 = np.mat(dZ1)\n",
    "        dW1 += np.dot(np.mat(Xb).T, dZ1[:,:-1])  # 3x3\n",
    "        # print(dZ2)\n",
    "\n",
    "        # Update the weights\n",
    "        W1 -= learning_rate * dW1 \n",
    "        W2 -= learning_rate * dW2\n",
    "        # Print the loss\n",
    "        if print_loss and iter % 1000 == 0:\n",
    "            print(\"Loss after iteration %i: %f\" % (iter, lossTemp))\n",
    "    return loss, W1, W2\n",
    "\n",
    "\n",
    "# Train the model\n",
    "loss, W1, W2 = train(\n",
    "    X, y, W1, W2, learning_rate=0.01, num_iterations=10000, print_loss=True\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with the parameters, of course. Especially the learning rate, and see how it affects the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9ea806aac0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the loss over time\n",
    "_, probs = forward(X, W1, W2)\n",
    "# \n",
    "print(np.around(probs) == y)\n",
    "plt.plot(loss)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generally plot this error over time so we make sure that the training process is working as intended. Of course, we don't actually need 10000 iterations, so that's why you should tweak the parameters some more. In practice, often times the architecture / method itself is not the faulty part, but the parameters.  \n",
    "### Always tweak 1 parameter at a time, and make sure to properly analyze the results.  \n",
    "It's a general fact that changing more than 1 thing at a time makes it impossible to be sure which changed what. Bonus tip: make sure that no random factors are involved in the tweaking process. For example, initializing the weights with random values is a random factor, so make sure to reseed and initialize with the same weights when testing how a different value for a parameter influences the results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap  \n",
    "  \n",
    "#### Network representation  \n",
    "  \n",
    "You can choose any kind of representation you want, of course. But we want to make our work easier, so we generally use matrices, due to the operations they allow. Frameworks have their own internal representation, but we use numpy arrays and matrices to demonstrate the concepts.  \n",
    "  \n",
    "So the input layer looks like this:  \n",
    "$$\\begin{bmatrix} x_1 \\ x_2 \\ \\dots \\ x_n \\ 1 \\end{bmatrix}$$  \n",
    "We chose to list them horizontally because each row will then represent a training example, and we can add the bias term here.  \n",
    "\n",
    "As we mentioned, we choose to integrate the bias terms into the input/weights/hidden nodes matrices. This means we make calculations easier on the forward pass. In the backpropagation process, we just have to make sure that we omit the bias terms when calculating gradients, so that we keep our matrices sizes consistent.  \n",
    "\n",
    "The hidden layers look like this:\n",
    "$$\\begin{bmatrix} h_1 \\ h_2 \\ \\dots \\ h_m \\ 1 \\end{bmatrix}$$  \n",
    "We added the bias term, don't forget the $h_i$ values are the outputs of the activation function.  \n",
    "\n",
    "The weights are represented as matrices, so a general representation would look like this:\n",
    "$$\\begin{bmatrix} w_{11} \\ w_{12} \\ \\dots \\ w_{1m} \\ w_{1b} \\\\ w_{21} \\ w_{22} \\ \\dots \\ w_{2m} \\ w_{2b} \\\\ \\vdots \\\\ w_{n1} \\ w_{n2} \\ \\dots \\ w_{nm} \\ w_{nb} \\end{bmatrix}$$  \n",
    "The bias terms are the last column. The first index represents the node the weight starts from, and the second index represents the node the weight ends at.  \n",
    "\n",
    "The output layer is the only one that doesn't have a bias term, so it looks like this:\n",
    "$$\\begin{bmatrix} o_1 \\ o_2 \\ \\dots \\ o_k \\end{bmatrix}$$  \n",
    "\n",
    "Of course, feeding multiple training examples at once is a thing, so we get multiple output rows this way. But in the training process, we can either use the whole dataset at once, or we can use mini-batches. That is a decision we make, with the goal of speeding up the process OR raising our accuracy chances.\n",
    "\n",
    "#### Forward propagation\n",
    "\n",
    "We use the formulas we presented earlier (which ammounts to dot products and activation functions) to generate the output. In the case of classification problems, we use sigmoid (for binary classification with a single output) and softmax (for multiclass classification with multiple outputs).  \n",
    "\n",
    "#### Backpropagation\n",
    "\n",
    "We use the formulas we presented earlier (which ammounts to derivatives of the activation functions and the cost function) to calculate the gradients of the cost function with respect to the weights. We then use these gradients to update the weights.  \n",
    "During this process, we make the decision to integrate the bias terms into the weights or keep it separate, to use mini-batch / batch / stochastic gradient descent, and to use the cost function we want."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The types of gradient descent \n",
    "\n",
    "Let's recap the types, their strengths and weaknesses.\n",
    "\n",
    "**Batch gradient descent**:\n",
    "- uses the whole dataset to calculate the gradient\n",
    "- slow\n",
    "- more accurate\n",
    "\n",
    "**Stochastic gradient descent**:\n",
    "- uses 1 sample to calculate the gradient\n",
    "- fast\n",
    "- less accurate (stumbles around, but overall in the direction of the minimum)\n",
    "\n",
    "**Mini-batch gradient descent**:\n",
    "- uses a subset of the dataset to calculate the gradient\n",
    "- somewhere in between the other 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing batch, mini-batch and stochastic gradient descent using scikit\n",
    "# Batch\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=(3),\n",
    "    activation=\"relu\",\n",
    "    solver=\"sgd\",\n",
    "    learning_rate_init=0.1,\n",
    "    max_iter=1000,\n",
    "    batch_size=4,\n",
    ")\n",
    "model.fit(X, y)\n",
    "batchErrors = model.loss_curve_\n",
    "# Mini-batch\n",
    "model2 = MLPClassifier(\n",
    "    hidden_layer_sizes=(3),\n",
    "    activation=\"relu\",\n",
    "    solver=\"sgd\",\n",
    "    learning_rate_init=0.1,\n",
    "    max_iter=1000,\n",
    "    batch_size=2,\n",
    ")\n",
    "model2.fit(X, y)\n",
    "minibatchErrors = model2.loss_curve_\n",
    "# Stochastic\n",
    "model3 = MLPClassifier(\n",
    "    hidden_layer_sizes=(3),\n",
    "    activation=\"relu\",\n",
    "    solver=\"sgd\",\n",
    "    learning_rate_init=0.1,\n",
    "    max_iter=1000,\n",
    "    batch_size=1,\n",
    ")\n",
    "model3.fit(X, y)\n",
    "stochasticErrors = model.loss_curve_\n",
    "plt.plot(stochasticErrors, label=\"Stochastic\", linestyle=\"--\")\n",
    "plt.plot(batchErrors, label=\"Batch\", linestyle=\":\")\n",
    "plt.plot(minibatchErrors, label=\"Mini-batch\", linestyle=\"-.\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used dashed lines because they're often positioned on top of each other. The learning rate is an important factor here, try to play around with the values and see if it makes a difference (a bigger learning rate might lead to failure of convergence, or to a faster one)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commonMLalgs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "500cef33aa921c87c63c1753d003ac956bc931603b9c07a450ae237ef80e36a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
