{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "\n",
    "Now this is the big topic of the last few decades. Since the 2010s, neural networks have become the most popular method applied in the field of ML. Why is that? We'll talk a bit about the history of neural networks, what they needed and lacked and what changed.\n",
    "\n",
    "#### History\n",
    "\n",
    "Neural networks were first introduced in the 1940s by Warren McCulloch and Walter Pitts. They proposed a simple electrical circuit that could be used to model the behavior of neurons. Later, the idea that neurons' pathways strengthen over repeated use, especially between neurons that fire together (Donald Hebb). That was the beginning of mapping the brain to a computer.  \n",
    "\n",
    "The start of the neural networks algorithm came with Frank Rosenblatt in 1957. He proposed the perceptron, a simple neural network that could be used to classify linearly separable data. The perceptron was a single layer neural network with a single output neuron. The input was processed as a weighted sum; then a threshold was applied, and the output was either 0 or 1.\n",
    "\n",
    "![An image of the perceptron; it has an input layer, does a weighted sum over the numbers and applies a threshold to determine the output as 1 or 0](https://miro.medium.com/max/1400/1*ofVdu6L3BDbHyt1Ro8w07Q.png)  \n",
    "Via [Towards Data Science](https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Frosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a&psig=AOvVaw3D5MgtSVmqF_YPMurYUCxv&ust=1674493057232000&source=images&cd=vfe&ved=0CA8QjRxqFwoTCIj4pfDS2_wCFQAAAAAdAAAAABAg)  \n",
    "\n",
    "The goal of the perceptron was to \"learn\" the weights of the linear combination, in order to minimize the difference between predicted output and desired output. Unfortunately, the perceptron could only learn linearly separable data, due to the linear processing applied. Another problem came with the scale of the model itself. By adding multiple layers, the model could learn more complex functions, and finally separate non-linear data. The problem, however, was the sheer complexity of the training task. At the time, the hardware and the methods applied were indicative of the fact that neural networks would lead to nowhere. That's how the first \"AI winter\" came to be, as AI research was abandoned for a while.  \n",
    "\n",
    "In the 1980s, the next efforts to bring AI forward began. Backpropagation had been implemented in the late 1960s, but it was only in the 1980s that people thought to use it for neural networks (we will discuss backpropagation in a future section). By this time, the so-called \"Expert system\" algorithm was adopted by many companies, relying on sets of rules to solve problems and make decisions. Around 1990, the fall of export systems brought a 2nd AI winter, although a shorter one. The developments of multi-layer perceptrons powered by the backpropagation algorithm kept going. The problem was with hardware capabilities. These \"slow learners\" relied on many, many iterations to reach good solutions. Computers needed to go faster. As such, progress was slow, dependent on the speed of the processors.  \n",
    "\n",
    "In the 2000s, the \"AI summer\" came. The development of GPUs (graphics processing units) allowed for a huge increase in the speed of neural networks. The first GPU was released in 1999, and by 2006, the first GPU-based neural network was developed. GPUs' role is to compute a lot of math at the same time. As it so happens, this is exactly what networks need. As such, the speed of the training process increased by a considerable order of magnitude. The development of the internet and the spreading availability of datasets (data) allowed for the development of deep learning, which is the name given to neural networks with many layers (depends on each person, but generally networks with more than 3 hidden layers are considered deep).  \n",
    "In the 2010s, GPUs leaped in performance, cementing the research of neural networks at the front of the AI industry. With time, specialized hardware was introduced, such as TPUs (Tensor Processing Units) and (lately) ML accelerators on mobile devices.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Now that we know a bit about how neural networks came to be, let's talk about what they are and how they work. We've mentioned them before, so now we'll go into detail.  \n",
    "\n",
    "A neural network is model that tries to mimic the human brain. Instead of actual neurons, we're using numbers and functions to achieve results. Each cell in a network is a **neuron**. The connections represent the pathways between neurons. In ML, they are called **weights** (mathematically), or **edges** (graphically, coming from the discipline of graphs). The neurons are organized in layers (input, hidden, output), and we apply **activation functions** after each layer's values have been computed. These activation functions are used to introduce non-linearity into the network (we'll see in more detail as we implement a network).  \n",
    "\n",
    "![A simple network with 3 input neurons connected to a hidden layer with 2 neurons and an output layer with 2 neurons. Each layer's neurons are fully connected to the next batch](../assets/simpleNN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
