{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Mathematics behind Support Vector Machines\n",
    "\n",
    "It's a big topic, so get confortable. We'll start with the prerequisites:  \n",
    "\n",
    "* Linear Algebra\n",
    "* Calculus  \n",
    "\n",
    "You would need these at a college-level understanding.\n",
    "\n",
    "You should understand these concepts:\n",
    "- Vectors & Matrices\n",
    "- Derivatives + partial derivatives\n",
    "- Distances in n-dimensional space\n",
    "- Function optimization with/without constraints (Lagrange multipliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the basic idea of SVM classifiers is the maximize the margin (distance from the hyperplane to the nearest data points) between the two classes. Let's think about how this is calculated.  \n",
    "\n",
    "Let's say we have a hyperplane. We calculate the distance to the closest data point (because that datapoint does in fact represent a sort of barrier we cannot cross - at least while we're talking about hard-margin classifiers). If we double that distance, we have our margin.  \n",
    "Of course, we want to maximize our margin, which means that our goal is to find a middle ground between the two classes (which leads to the maximum possible margin).  \n",
    "To do that, we introduce a few ideas:  \n",
    "- each point in n-space is essentially a vector.\n",
    "- our hyperplane is an (n-1)-dimensional space, which means that it is a vector in n-space (because we add back the intercept-term).  \n",
    "\n",
    "So we have 2 vectors: a point A, and our normal vector W (which describes the hyper-plane; remember that this vector will always be normal to the actual hyperplane representation in a graph). To get the distance between the hyperplane and the point A, we have to *project* A onto W, then calculate the norm of that projection. Double that, and we have our margin calculated. The steps are relatively simple, and the math is easy to apply, once we get the hang of it.  \n",
    "\n",
    "Formula for projection of A onto W (let **U** be the unit vector of W, *aka* the direction of W; let **P** be the projection):\n",
    "$$\n",
    "U = \\frac{W}{||W||}\n",
    "$$\n",
    "$$\n",
    "||P|| = U \\cdot A\n",
    "$$  \n",
    "$$\n",
    "P = (U \\cdot A) \\cdot U\n",
    "$$\n",
    "\n",
    "\n",
    "Formula for norm:\n",
    "$$\n",
    "\\| \\vec{A} \\| = \\sqrt{\\vec{A} \\cdot \\vec{A}}\n",
    "$$\n",
    "\n",
    "Note that we would only be interested in the norm of the projection, not the projection itself. We would simply calculate $||P|| = U \\cdot A$, and double it (because this value is only one side of the hyperplane, and we want to take into account both sides). That would be our margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know how to calculate the margin for one hyperplane. Let's see how we can find the optimal hyperplane, with the biggest margin.  \n",
    "\n",
    "Since our goal is to find the biggest value for the margin, this means we find the biggest norm for $P$, our projection vector. As we now know, $||P|| = U \\cdot A$.  \n",
    "$U = \\frac{W}{||W||}$, so we can substitute that into the equation for $||P||$. \n",
    "$$\n",
    "||P|| = \\frac{W}{||W||} \\cdot A\n",
    "$$\n",
    "$$\n",
    "||P|| = \\frac{W \\cdot A}{||W||}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The math does involve some more steps, but I'm going to simplify it a bit.  \n",
    "If you want to see each step of the way, I would recommend checking out this [article](https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-2/). You might want to read every part, it's worth it.  \n",
    "\n",
    "All right, the simplification is as follows: to get $||P||$ as big as possible, we look at our variables. We have $W$, that's our hyperplane vector. We change this, it's norm changes. We have a few ways in which we could change it, but which one is the best? Let's go to $A$. This is a datapoint, so we know for sure we're not modifying anything about that. Lastly, we have $||W||$, which depends on $W$. What's different now is that instead of having so many directions in which we could change $W$, we only have 1 for $||W||$: make it smaller. To maximize $||P||$, we want to make $||W||$ as small as possible. And that gives us a target:  \n",
    "\n",
    "#### Minimize the norm of our weights vector W\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
