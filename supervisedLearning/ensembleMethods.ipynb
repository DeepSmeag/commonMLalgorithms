{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "\n",
    "In this chapter, we will discuss ensemble methods in-depth: why they exists, what they have to offer and where to use them.  \n",
    "Ensemble methods are a type of machine learning algorithm that combines multiple base models (we call them *\"weak learners\"*) in order to produce one optimal predictive model (*\"strong learners\"*). Ensemble methods are very popular in machine learning competitions because they often produce the best predictive models. In this chapter, we will cover the following subjects:\n",
    "- Bagging\n",
    "- Boosting\n",
    "- Stacking  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Ensemble Methods?\n",
    "\n",
    "As we've mentioned above, ensemble methods often produce the best results. Let's go back to our *1000 people* example. We know through common sense that askins a lot of people for their opinion on a topic will often lead to a better result than asking only one person. But why?  \n",
    "\n",
    "There are a few reasons. Of course, we're not citing research right now, but think about it: different people means different skillsets, different background, different experiences, different viewpoints. By attacking a problem from so many different angles, we're bound to reach a results that's close to the truth (or at least closer than if we only had one person's opinion).  \n",
    "\n",
    "This variety is what gives ensemble methods their power. We combine the opinions of many different models to reach a more accurate, more consistent result."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we didn't use multiple models?\n",
    "\n",
    "Well, we've already seen what happens. When forced to resort to a single model, we have to make multiple decisions. These are failure points. Each decision is something that requires a human to make a correct prediction about the best direction the model should take.  \n",
    "\n",
    "Things like data availability or cleanliness (think clean = organized, tabular data; dirty = random words on a page), feature engineering (choosing which parts of the data should be used where), model selection (thinking about and choosing a model that can describe the data accurately) and hyperparameter tuning (experimenting with different values and picking the best combination) are all decisions that can lead to failure.  \n",
    "\n",
    "In the end, we have no choice but to make decisions ourselves about those variables, and we're bound to make mistakes. Those mistakes add up. That's why ensemble methods come in handy: they allow us to average out the mistakes made by each model, so that the overall result is more accurate than any single model could have produced.  \n",
    "\n",
    "Another way to think about it: we've discussed the [bias-variance tradeoff](../otherConcepts/bias%26variance.ipynb). Choosing one single model forces us to have this tradeoff. Averaging across multiple models allows us to reduce the variance of our predictions, which is a good thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commonMLalgs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13 (main, Oct 13 2022, 21:15:33) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "500cef33aa921c87c63c1753d003ac956bc931603b9c07a450ae237ef80e36a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
