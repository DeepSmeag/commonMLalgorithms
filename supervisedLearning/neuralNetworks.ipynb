{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "\n",
    "Now this is the big topic of the last few decades. Since the 2010s, neural networks have become the most popular method applied in the field of ML. Why is that? We'll talk a bit about the history of neural networks, what they needed and lacked and what changed.\n",
    "\n",
    "#### History\n",
    "\n",
    "Neural networks were first introduced in the 1940s by Warren McCulloch and Walter Pitts. They proposed a simple electrical circuit that could be used to model the behavior of neurons. Later, the idea that neurons' pathways strengthen over repeated use, especially between neurons that fire together (Donald Hebb). That was the beginning of mapping the brain to a computer.  \n",
    "\n",
    "The start of the neural networks algorithm came with Frank Rosenblatt in 1957. He proposed the perceptron, a simple neural network that could be used to classify linearly separable data. The perceptron was a single layer neural network with a single output neuron. The input was processed as a weighted sum; then a threshold was applied, and the output was either 0 or 1.\n",
    "\n",
    "![An image of the perceptron; it has an input layer, does a weighted sum over the numbers and applies a threshold to determine the output as 1 or 0](https://miro.medium.com/max/1400/1*ofVdu6L3BDbHyt1Ro8w07Q.png)  \n",
    "Via [Towards Data Science](https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Frosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a&psig=AOvVaw3D5MgtSVmqF_YPMurYUCxv&ust=1674493057232000&source=images&cd=vfe&ved=0CA8QjRxqFwoTCIj4pfDS2_wCFQAAAAAdAAAAABAg)  \n",
    "\n",
    "The goal of the perceptron was to \"learn\" the weights of the linear combination, in order to minimize the difference between predicted output and desired output. Unfortunately, the perceptron could only learn linearly separable data, due to the linear processing applied. Another problem came with the scale of the model itself. By adding multiple layers, the model could learn more complex functions, and finally separate non-linear data. The problem, however, was the sheer complexity of the training task. At the time, the hardware and the methods applied were indicative of the fact that neural networks would lead to nowhere. That's how the first \"AI winter\" came to be, as AI research was abandoned for a while.  \n",
    "\n",
    "In the 1980s, the next efforts to bring AI forward began. Backpropagation had been implemented in the late 1960s, but it was only in the 1980s that people thought to use it for neural networks (we will discuss backpropagation in a future section). By this time, the so-called \"Expert system\" algorithm was adopted by many companies, relying on sets of rules to solve problems and make decisions. Around 1990, the fall of export systems brought a 2nd AI winter, although a shorter one. The developments of multi-layer perceptrons powered by the backpropagation algorithm kept going. The problem was with hardware capabilities. These \"slow learners\" relied on many, many iterations to reach good solutions. Computers needed to go faster. As such, progress was slow, dependent on the speed of the processors.  \n",
    "\n",
    "In the 2000s, the \"AI summer\" came. The development of GPUs (graphics processing units) allowed for a huge increase in the speed of neural networks. The first GPU was released in 1999, and by 2006, the first GPU-based neural network was developed. GPUs' role is to compute a lot of math at the same time. As it so happens, this is exactly what networks need. As such, the speed of the training process increased by a considerable order of magnitude. The development of the internet and the spreading availability of datasets (data) allowed for the development of deep learning, which is the name given to neural networks with many layers (depends on each person, but generally networks with more than 3 hidden layers are considered deep).  \n",
    "In the 2010s, GPUs leaped in performance, cementing the research of neural networks at the front of the AI industry. With time, specialized hardware was introduced, such as TPUs (Tensor Processing Units) and (lately) ML accelerators on mobile devices.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Now that we know a bit about how neural networks came to be, let's talk about what they are and how they work. We've mentioned them before, so now we'll go into detail.  \n",
    "\n",
    "A neural network is model that tries to mimic the human brain. Instead of actual neurons, we're using numbers and functions to achieve results. Each cell in a network is a **neuron**. The connections represent the pathways between neurons. In ML, they are called **weights** (mathematically), or **edges** (graphically, coming from the discipline of graphs). The neurons are organized in layers (input, hidden, output), and we apply **activation functions** after each layer's values have been computed. These activation functions are used to introduce non-linearity into the network (we'll see in more detail as we implement a network).  \n",
    "\n",
    "![A simple network with 3 input neurons connected to a hidden layer with 2 neurons and an output layer with 2 neurons. Each layer's neurons are fully connected to the next batch](../assets/simpleNN.png)  \n",
    "\n",
    "Let's disect this image, shall we?  \n",
    "##### Input layer\n",
    "We have neurons $n_1, n_2, n_3$ in the input layer. These receive the input data. The features we've been using so far are these neurons' values. Quite simple and direct so far. We now have to use and combine them somehow to reach useful information. The weights are the connections between neurons of consecutive layers. These denote the \"importance\" of any given neuron's value.\n",
    "\n",
    "##### Hidden layer\n",
    "These are the intermediary layers that do the bulk of the processing. Let's jump to an example. Using the diagram above, the 2 neurons in the hidden layer would receive the following values:\n",
    "$$\n",
    "\\begin{align}\n",
    "n_{1, hidden} &= w_{1, 1} \\cdot n_1 + w_{2, 1} \\cdot n_2 + w_{3, 1} \\cdot n_3 \\\\\n",
    "n_{2, hidden} &= w_{1, 2} \\cdot n_1 + w_{2, 2} \\cdot n_2 + w_{3, 2} \\cdot n_3\n",
    "\\end{align}\n",
    "$$\n",
    "I haven't numbered them, since it won't be that important. What matters is that we observe the following fact: what we're doing is essentially a linear combination of the input values. Remember when we discussed linear regression? We are following the same steps here. But something is missing: the bias term. Yes, we need it, as we've needed it before. Just as a linear function would not be able to fit to data that does not go through the origin, a neural network cannot fit to certain types of data without the bias term. In terms of how neural networks cover this need, we could have an additional neuron on each layer with a constant value of 1. The weight associated with that neuron would be the bias term.\n",
    "\n",
    "##### Output layer\n",
    "I have drawn 2 neurons in the output layer. It's important to introduce this fact early on: neural networks can produce any number of outputs, so it's a matter of the problem at hand and the result we want to achieve. After the computing has been done, these neurons will hold the final result: the output of the neural network.\n",
    "\n",
    "### Warning: activation functions\n",
    "I would like you to consider the following issue: as we've seen, going from one layer to the next involves computing a linear combination of the values in the previous layer. Apply this any number of times, but in the end what you get is still a linear combination. We know for a fact that a linear combination outputs a line, no matter how many times we apply it. This is problematic. We need to introduce non-linearity into the network, in order to help it fit data in high-dimensional space that is not linearly separable. This is why **activation functions** exist. They are applied after each layer's values have been computed.  \n",
    "\n",
    "There are lots of activation functions out there, and we could in theory use anything, but the purpose is to introduce non-linearity with the cheapest computational cost possible. Here are the most common ones:\n",
    "- Sigmoid  \n",
    "![sigmoid](https://miro.medium.com/max/640/1*Xu7B5y9gp0iL5ooBj7LtWw.webp)  \n",
    "Via [Towards Data Science](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)\n",
    "- Tanh  \n",
    "![tanh](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_4.23.22_PM_dcuMBJl.png)  \n",
    "Via [Papers with code](https://paperswithcode.com/method/tanh-activation)\n",
    "- ReLU  \n",
    "![relu](https://www.nomidl.com/wp-content/uploads/2022/04/image-10.png)  \n",
    "Via [Nomidl](https://www.nomidl.com/deep-learning/what-is-relu-and-sigmoid-activation-function/)\n",
    "- Leaky ReLU  \n",
    "![leaky relu](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_3.09.45_PM.png)  \n",
    "Via [Papers with code](https://paperswithcode.com/method/leaky-relu)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anything that isn't a simple $f(x) = x$ will do. But we have to be careful about the computation involved. This is why the industry turned to ReLU, for it's simplicity and speed. We've talked about gradient descent before. Imagine calculating the derivative of Tanh vs ReLU. The former is a lot more complex, and the latter is a simple $f(x) = 1$ if $x > 0$, and $f(x) = 0$ otherwise. This is why ReLU is so popular. It really might be the most popular out there. We'll talk about the *vanishing gradient* problem after discussing the training of neural networks, and we'll see why sigmoid and tanh are not used as much.  \n",
    "\n",
    "### Some more details\n",
    "\n",
    "There are a few things one should know about neural networks, right from the beginning. They can be arranged in a multitude of ways, which is why lots of architectures exist. We call nets in which all neurons are connected as *fully connected*. We'll see that there are problems that benefit from designing a network with a different topologies.  \n",
    "The more layers & neurons we have, the more complex the data we can fit. The great thing about networks is that they really excel at finding patterns in highly-dimensional, highly-complex data. They don't require human intervention all that much. If we see signs of high bias (underfitting), we can simply add more neurons (existing or new layers). This increases complexity. The problem then is to gather enough data to be able to train these. They are prone to overfitting without a lot of data, since they are so great at fitting despite complexity.  \n",
    "\n",
    "One thing to keep in mind: the parameters we train with NNs are the *weights*. Neurons are simply information-holding cells. Activation functions are static procedures we apply to values. There is nothing we should be modifying except for weights. Now, consider the number of the weights. They are parameters, so for each one we add, we increase the required compute power. In the diagram above, we have 10 weights. That's 10 parameters to tweak. Add the bias terms (in practice, you really should), and we get another 4, so that's 14 total. Things get really complicated really fast."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: how do NNs work?\n",
    "\n",
    "We get our features/values into the input layer.  \n",
    "Those values are part of linear combinations, which go to the next layer.  \n",
    "Each neuron in that next layer applies an \"activation function\" (a non-linear function), which updates the values.  \n",
    "Those are the final values of that layer.  \n",
    "The next step is to repeat the process for the next layer, until we reach the output layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training: How do we find the optimal parameters?\n",
    "\n",
    "Starting with the basics, neural networks are a supervised algorithm. This means we know the output. This also means we can define error/loss functions. These tell us how far from the real answer we are.  \n",
    "The next step is to define a method that brings us closer to the right values for the parameters. We could guess and check. Imagine how slow that would go, based on the fact that a simple network like the one above should have 14 parameters to tweak. So then we use math. We turn to *gradient descent* and *backpropagation*.  \n",
    "\n",
    "## Gradient descent\n",
    "\n",
    "Gradient descent is a method that allows us to find the minimum of a function, by continuously updating our parameters to go towards the direction of the steepest descent. We've already seen it in action, so the new element is backpropagation. Let's check the simple network again:  \n",
    "![The same network from before](../assets/simpleNN.png)  \n",
    "Our parameters are the weights between the neurons. Our error is after the output layer. We can reach the weights just before the output layer, since we know the steps that were applied to calculate our final output.  \n",
    "We will make some notations:  \n",
    "$g_1(x) = w_1x + b_1$  \n",
    "$g_2(x) = w_2x + b_2$  \n",
    "Let's call our output neurons $o_1$ and $o_2$. Their values would be:  \n",
    "$o_1 = f(g_1(x))$  \n",
    "$o_2 = f(g_2(x))$  \n",
    "\n",
    "Where we would have $f$ as the activation function, $w_i$ as the corresponding set of weights, $b_i$ as the bias terms, and $x$ as the values of the previous layer of neurons.  \n",
    "\n",
    "By applying our error function to these $o_i$ values, we get the error. In order to reach the error with respect to the weights, we need to get through some steps. We apply *the chain rule*:  \n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_1} = \\frac{\\partial E}{\\partial o_1} \\frac{\\partial o_1}{\\partial w_1} = \\frac{\\partial E}{\\partial o_1} \\frac{\\partial o_1}{\\partial g_1} \\frac{\\partial g_1}{\\partial w_1}\n",
    "\\\\\n",
    "\\text{} \\\\\n",
    "\\\\\n",
    "\\frac{\\partial E}{\\partial w_2} = \\frac{\\partial E}{\\partial o_2} \\frac{\\partial o_2}{\\partial w_2} = \\frac{\\partial E}{\\partial o_2} \\frac{\\partial o_2}{\\partial g_2} \\frac{\\partial g_2}{\\partial w_2}\n",
    "$$  \n",
    "\n",
    "As you can see, terms can be nullified if we're looking diagonally."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A bit of math\n",
    "\n",
    "We call it the chain rule because it helps us get through the chain of operations that led to the final output. We can calculate how much each part of the whole equation contributed to the final error.  \n",
    "In order to properly calculate some values (or get some equations), we have to choose our functions. Let's suppose our NN classifies some data. Both our outputs would be a 1 or a 0. For the error function, we'll choose the squared error, and the activation function will be ReLU.  \n",
    "\n",
    "Let's list them:\n",
    "$$\n",
    "\\begin{align}\n",
    "E &= \\frac{1}{2}(o_1 - y_1)^2 + \\frac{1}{2}(o_2 - y_2)^2 \\\\\n",
    "o_1 &= f(w_1x + b_1) \\\\\n",
    "o_2 &= f(w_2x + b_2) \\\\\n",
    "f(x) &= \\begin{cases} 0 & x < 0 \\\\ x & x \\geq 0 \\end{cases} \\\\\n",
    "o_i &= \\begin{cases} 0 & w_ix + b_i < 0 \\\\ w_ix + b_i & w_ix + b_i \\geq 0 \\end{cases} \\\\\n",
    "\\end{align}\n",
    "$$  \n",
    "\n",
    "So then, let's consider our first weight, $w_1$. Here are our derivatives:  \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial o_1} &= o_1 - y_1 \\\\\n",
    "\\frac{\\partial o_1}{\\partial w_1} &= \\begin{cases} 0 & w_1x + b_1 \\leq 0 \\\\ x & w_1x + b_i > 0 \\end{cases} \\\\\n",
    "\n",
    "\\frac{\\partial E}{\\partial w_1} &= (o_1 - y_1) * \\frac{\\partial o_1}{\\partial w_1} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\text{Note: Derivative of ReLU is undefined in 0, but we can set it to 0, since that would be the value of the function in that point.} \\\\\n",
    "$$\n",
    "\n",
    "\n",
    "Of course, in these formulas, x would be a vector, which is not the right way to go about this. We considered $w_1$ and $w_2$ as sets of weights. In practice, we take each constituent weight and calculate the error with respect to it. The $x$ would be the corresponding output of the previous neuron. Note: we also calculate the error with respect to the bias terms. They don't get the $x$ in the formula.  \n",
    "\n",
    "Let's suppose we did our math correctly, we got the error with respect to the weights just before the output layer. As we know, GD tells us to now update their values by subtracting the error times a learning rate. We know the drill by know. But...  \n",
    "\n",
    "What about the previous layers? Well, it turns out we do essentially the same steps, but more of them. This is where backpropagation comes in."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "Backpropagation is the process of propagating the error of the weights in the previous layers (back + propagation, makes sense right?). We do this by applying the chain rule. We start with the output layer, and we go backwards. We calculate the error of the weights just before the output layer, then we calculate the error of the weights just before that layer, and so on. In short, we apply gradient descent per layer going from output to input. Of course, the input layer has no weights to update, so we stop there.  \n",
    "\n",
    "As such, we call the prediction part of the network the *forward pass* (forward propagation), and the training part the *backward pass* (backpropagation).  \n",
    "\n",
    "As an example, the $w_11$ weight after the input layer would have the following error:  \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial w_{11}} &= \\frac{\\partial E}{\\partial o_1} \\frac{\\partial o_1}{\\partial w_{11}} + \\frac{\\partial E}{\\partial o_2} \\frac{\\partial o_2}{\\partial w_{11}} \\\\\n",
    "&= (o_1 - y_1) \\frac{\\partial o_1}{\\partial w_{11}} + (o_2 - y_2) \\frac{\\partial o_2}{\\partial w_{11}} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $o_i$ is the output of a neuron in the output layer.  \n",
    "We're missing the derivatives of $o_i$ with respect to $w_{11}$. But $w_{11}$ is not directly linked to any of the outputs. How do we calculate this, then?  \n",
    "We're going to go in-depth, since this seems to be the only way to make sure that everything makes sense."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop, step by step\n",
    "\n",
    "In order to do this properly, we'll need a more detailed diagram of our network. This time, each notation is important so we can keep track of what's happening.  \n",
    "\n",
    "![Diagram of neural network with notations in order. Input has 3 neurons. There is 1 hidden layer with 2 neurons. Output has 2 neurons. Bias is displayed for input and hidden layer. Notations for weights are w with i and j indices, where i is the index of the neuron in the 1st layer, and j is the index of the neuron in the 2nd layer. Weights also have a superscript, which is the index of the layer they go into. We've considered the hidden layer as layer 1. Bias is denoted by a b with an index and a superscript with the layer. Notation before activation function of a layer is H index i and superscript with layer. The hidden layer is denoted by Z and index after activation function. The output layer just has the notation O after the activation function.](../assets/simpleNNdetailed.png)  \n",
    "This one diagram is going to help us make sense of everything that is going on. I would recommend drawing it on a piece of paper before going forward.  \n",
    "Let's first go through what each notation means.  \n",
    "\n",
    "Input: we have 3 input variables, $x_1$, $x_2$ and $x_3$. This time, we have drawn the bias terms. We consider a fictitious input that has a constant value of 1. If we do the calculations normally, we just multiply the bias by 1, which leaves the bias term unchanged. This helps us scale computation later on.  \n",
    "The indexes correspond to the neurons in each pair of layers. The superscript is the index of the layer that the weight goes into (if we had multiple hidden layers, we would have superscripted the z's as well). We consider the hidden layer as layer 1.  With these notations in mind, you should be able to trace out the forward pass through the network. We're going to go through the backward pass together.  \n",
    "\n",
    "##### Note: Any kind of number a variable influences, that number can be partially derived with respect to that variable. No matter how many things were added on top, we can always use the chain rule to get back to that one variable. You will see this in action as we go.\n",
    "\n",
    "A reminder on some notations and formulas we use:\n",
    "$$\n",
    "\\begin{align}\n",
    "h_1^{(1)} &= \\sum_{i=1}^3 w_{i1}^{(1)}x_i + b_1^{(1)} \\\\\n",
    "z_1 &= f(h_1^{(1)}) \\\\\n",
    "f(x) &= \\begin{cases} 0 & x < 0 \\\\ x & x \\geq 0 \\end{cases} \\\\\n",
    "h_1^{(2)} &= \\sum_{i=1}^2 w_{i1}^{(2)}z_i + b_1^{(2)} \\\\\n",
    "o_1 &= f(h_1^{(2)}) \\\\\n",
    "E &= \\frac{1}{2} \\sum_{i=1}^2 (o_i - y_i)^2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\text{$y_i$ is the ground truth for the i-th output neuron.} \\\\\n",
    "$$  \n",
    "The rest of the terms are calculated similarly to these.  \n",
    "As we go through the process, I highly recommend writing everything down. It helps with understanding."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the error of the output layer\n",
    "\n",
    "$$\n",
    "E = \\frac{1}{2} \\sum_{i=1}^2 (o_i - y_i)^2 \\\\\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial o_1} &= o_1 - y_1 \\\\\n",
    "\\frac{\\partial E}{\\partial o_2} &= o_2 - y_2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Going further:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial h_1^{(2)}} &= \\frac{\\partial E}{\\partial o_1} \\frac{\\partial o_1}{\\partial h_1^{(2)}} \\\\\n",
    "\\frac{\\partial o_1}{\\partial h_1^{(2)}} &= \\begin{cases} 1 & h_1^{(2)} > 0 \\\\ 0 & h_1^{(2)} \\leq 0 \\end{cases} \\\\\n",
    "\\frac{\\partial E}{\\partial h_1^{(2)}} &= (o_1 - y_1) ,&  h_1^{(2)} > 0\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "We're supposing that these branches go in the \"$> 0$\" direction, since otherwise it's not that useful. This helps with making things clearer.\n",
    "$\\frac{\\partial E}{\\partial h_2^{(2)}}$ is analogous.  \n",
    "\n",
    "Finally, going for the weights (the part that really interests us, since they are the variables we can actually change):\n",
    "$$\n",
    "\\begin{align}\n",
    "h_1^{(2)} &= \\sum_{i=1}^2 w_{i1}^{(2)}z_i + b_1^{(2)} \\\\\n",
    "\\frac{\\partial E}{\\partial w_{11}^{(2)}} &= \\frac{\\partial E}{\\partial h_1^{(2)}} \\frac{\\partial h_1^{(2)}}{\\partial w_{11}^{(2)}} \\\\\n",
    "\\frac{\\partial h_1^{(2)}}{\\partial w_{11}^{(2)}} &= z_1 \\\\\n",
    "\\frac{\\partial E}{\\partial w_{11}^{(2)}} &= (o_1 - y_1) z_1 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Similarly...\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial h_1^{(2)}}{\\partial b_1^{(2)}} &= 1 \\\\\n",
    "\\frac{\\partial E}{\\partial b_1^{(2)}} &= \\frac{\\partial E}{\\partial h_1^{(2)}} \\frac{\\partial h_1^{(2)}}{\\partial b_1^{(2)}} \\\\\n",
    "&= (o_1 - y_1) \\\\\n",
    "\\end{align}\n",
    "$$  \n",
    "\n",
    "So we know the derivatives of the error with respect to the weights and biases of the output layer. The formulas are similar for the others. We can now update their values, as per GD rules:\n",
    "$$\n",
    "\\begin{align}\n",
    "w_{11}^{(2)} &= w_{11}^{(2)} - \\alpha \\frac{\\partial E}{\\partial w_{11}^{(2)}} \\\\  \n",
    "b_1^{(2)} &= b_1^{(2)} - \\alpha \\frac{\\partial E}{\\partial b_1^{(2)}} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "##### In practice, we don't update the values right away, since these would be used further down the line. We could either store some values in temporary variables, or we could update them after calculating what's needed for the next layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the error of the hidden layer\n",
    "\n",
    "The first step is to calculate $\\frac{\\partial E}{\\partial z_1}$ and $\\frac{\\partial E}{\\partial z_2}$.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial z_1} &= \\frac{\\partial E}{\\partial h_1^{(2)}} \\frac{\\partial h_1^{(2)}}{\\partial z_1} \\\\\n",
    "\\frac{\\partial h_1^{(2)}}{\\partial z_1} &= w_{11}^{(2)} \\\\\n",
    "\\frac{\\partial E}{\\partial z_1} &= (o_1 - y_1) w_{11}^{(2)} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### !!\n",
    "But here is the thing: $z_1$ influences the output in both $o_1$ and $o_2$. So we need to add the influence of $z_1$ on $o_2$ as well. The formula above is incomplete, here is the correct version:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial z_1} &= \\frac{\\partial E}{\\partial h_1^{(2)}} \\frac{\\partial h_1^{(2)}}{\\partial z_1} + \\frac{\\partial E}{\\partial h_2^{(2)}} \\frac{\\partial h_2^{(2)}}{\\partial z_1}\\\\\n",
    "\\frac{\\partial h_2^{(2)}}{\\partial z_1} &= w_{12}^{(2)} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial z_1} &= (o_1 - y_1) w_{11}^{(2)} + (o_2 - y_2) w_{12}^{(2)} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "The same goes for $z_2$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial h_1^{(1)}} &= \\frac{\\partial E}{\\partial z_1} \\frac{\\partial z_1}{\\partial h_1^{(1)}} \\\\\n",
    "\\frac{\\partial z_1}{\\partial h_1^{(1)}} &= \\begin{cases} 1 & h_1^{(1)} > 0 \\\\ 0 & h_1^{(1)} \\leq 0 \\end{cases} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "(By now, you probably see why ReLU is so popular. No extra computation required).  \n",
    "\n",
    "Now we can move on to weights.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h_1^{(1)} &= \\sum_{i=1}^2 w_{i1}^{(1)}x_i + b_1^{(1)} \\\\\n",
    "\\frac{\\partial E}{\\partial w_{11}^{(1)}} &= \\frac{\\partial E}{\\partial h_1^{(1)}} \\frac{\\partial h_1^{(1)}}{\\partial w_{11}^{(1)}} \\\\\n",
    "\\frac{\\partial h_1^{(1)}}{\\partial w_{11}^{(1)}} &= x_1 \\\\\n",
    "\\frac{\\partial E}{\\partial w_{11}^{(1)}} &= ((o_1 - y_1) w_{11}^{(2)} + (o_2 - y_2) w_{12}^{(2)}) x_1 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Similarly, for the bias:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial h_1^{(1)}}{\\partial b_1^{(1)}} &= 1 \\\\\n",
    "\\frac{\\partial E}{\\partial b_1^{(1)}} &= \\frac{\\partial E}{\\partial h_1^{(1)}} \\frac{\\partial h_1^{(1)}}{\\partial b_1^{(1)}} \\\\\n",
    "&= ((o_1 - y_1) w_{11}^{(2)} + (o_2 - y_2) w_{12}^{(2)}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And that's it! The rest of the weights are analogous, so no new formulas are needed. If you understand why things work out the way they do, you can easily extend this process to any number of hidden layers. But we won't stop here. As you can see, the formulas require some information to be calculated, some to be stored, and some to be updated. As we scale to bigger networks, doing this by hand is too tedious. That's why we use matrices, vectors, and their associated operations to make things scale easily. We also see some terms being reused. Once we compute the derivatives with respect to the $h_i^{(l)}$'s of a layer, we do not have to go back to the output layer. It's like a savepoint in a game. This is why we introduce the following name: \"**local gradients**\".  \n",
    "\n",
    "Here is the idea and notation:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta_1^{(2)} &= \\frac{\\partial E}{\\partial h_1^{(2)}}\\\\\n",
    "\\delta_2^{(2)} &= \\frac{\\partial E}{\\partial h_2^{(2)}}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "These are the local gradients of the output layer. We already know how they are calculated. We use them to obtain the relevant information about derivatives with respect to the weights and biases of the 2nd layer. Once we continue our backpropagation process, we reach the following:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta_1^{(1)} &= \\frac{\\partial E}{\\partial h_1^{(1)}}\\\\\n",
    "\\delta_2^{(1)} &= \\frac{\\partial E}{\\partial h_2^{(1)}}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Again, we know how to calculate these. We just have to use them. As we've mentioned earlier, these values are reused, so we don't have to go back to the output layer. In implementation terms, we store the local gradients of layer $(l)$ before updating the weights of layer $(l+1)$. This is the way we guarantee that we don't use the updated weights to calculate the local gradients of layer $(l)$.  \n",
    "And with these in mind, backpropagation is even easier to understand and implement. We just start from the output layer, calculate our way backwards, and save progress as we go."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "As we know, vectorization allows for faster processing times (in general, due to parallelization). We'll see how dot products and transposes help us vectorize our code. \n",
    "Let's gather our matrices and vectors:\n",
    "\n",
    "$$\n",
    "\\text{Let $X$ be the input vector}\\\\\n",
    "X = \\begin{bmatrix}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "x_{3} \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\text{Let $W^{(1)}$ be the matrix of weights of the 1st layer}\\\\\n",
    "\\\\\n",
    "W^{(1)} = \\begin{bmatrix}\n",
    "w_{11}^{(1)} & w_{12}^{(1)} \\\\\n",
    "w_{21}^{(1)} & w_{22}^{(1)} \\\\ \n",
    "w_{31}^{(1)} & w_{32}^{(1)} \\\\\n",
    "b_{1}^{(1)} & b_{2}^{(1)} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\text{Let $W^{(2)}$ be the matrix of weights of the 2nd layer}\\\\\n",
    "\\\\\n",
    "W^{(2)} = \\begin{bmatrix}\n",
    "w_{11}^{(2)} & w_{12}^{(2)} \\\\\n",
    "w_{21}^{(2)} & w_{22}^{(2)} \\\\\n",
    "b_{1}^{(2)} & b_{2}^{(2)} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\text{Let $y$ be the vector of outputs}\\\\\n",
    "\\\\\n",
    "y = \\begin{bmatrix}\n",
    "y_{1} \\\\\n",
    "y_{2} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\text{Let $O$ be the vector of outputs}\\\\\n",
    "\\\\\n",
    "O = \\begin{bmatrix}\n",
    "o_{1} \\\\\n",
    "o_{2} \\\\\n",
    "\\end{bmatrix}\n",
    "$$  \n",
    " \n",
    "As you can see, we've added the bias terms to the weights matrices. To account for this, we add 1's to the input vector and the $Z^{(l)}$ vectors. This is the way we make sure that the bias terms are taken into account.  \n",
    "\n",
    "---\n",
    "\n",
    "$$\n",
    "\\text{$H^{(1)}$ is the vector of outputs of the 1st layer, and $H^{(2)}$ for the 2nd layer}\\\\\n",
    "\\\\\n",
    "H^{(1)} = \\begin{bmatrix}\n",
    "h_{1}^{(1)} \\\\\n",
    "h_{2}^{(1)} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "Z^{(1)} = \\begin{bmatrix}\n",
    "z_{1}^{(1)} \\\\\n",
    "z_{2}^{(1)} \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "H^{(2)} = \\begin{bmatrix}\n",
    "h_{1}^{(2)} \\\\\n",
    "h_{2}^{(2)} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "$$\n",
    "\n",
    "As per these notations, we can write the following:\n",
    "$$\n",
    "\\begin{align}\n",
    "H^{(1)} &= W^{(1)'} \\cdot X &\\text{' denotes the transpose}\\\\\n",
    "Z^{(1)} &= f(H^{(1)}) & \\text{where $f$ is the activation function, and we add the 1 after applying it} \\\\\n",
    "H^{(2)} &= W^{(2)'} \\cdot Z^{(1)} \\\\\n",
    "O &= f(H^{(2)}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "It's quite easy to follow these in the forward pass. With these notations, we can do the backpropagation process in a vectorized way.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta_1^{(2)} &= \\frac{\\partial E}{\\partial h_1^{(2)}} \\\\\n",
    "&= (o_1 - y_1) f'(h_1^{(2)}) \\\\\n",
    "\\delta_2^{(2)} &= \\frac{\\partial E}{\\partial h_2^{(2)}} \\\\\n",
    "&= (o_2 - y_2) f'(h_2^{(2)}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Is equivalent to:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta^{(2)} &= \\frac{\\partial E}{\\partial H^{(2)}} \\\\\n",
    "&= (O - Y) * f'(H^{(2)}) & \\text{with dimensions 2x1} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Where $Y$ is the vector of expected outputs, * means element-wise multiplication, and $f'$ is the derivative of the activation function.  \n",
    "\n",
    "The derivatives with respect to the weights and biases of the 2nd layer are:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial W^{(2)}} &= \\frac{\\partial E}{\\partial H^{(2)}}\\frac{\\partial H^{(2)}}{\\partial W^{(2)}} = Z^{(1)} \\cdot \\delta^{(2)'} & \\text{with dimensions 3x2, so we can subtract right away}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "We don't need separate derivatives with respect to the biases, because they are included in the W vector.\n",
    "\n",
    "Going to the 1st layer, we have:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta_1^{(1)} &= \\frac{\\partial E}{\\partial h_1^{(1)}} \\\\\n",
    "&= \\delta_1^{(2)} \\cdot w_{11}^{(2)} + \\delta_2^{(2)} \\cdot w_{12}^{(2)} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Which is equivalent to:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta^{(1)} &= \\frac{\\partial E}{\\partial H^{(1)}} \\\\\n",
    "&= W^{(2)} \\cdot \\delta^{(2)} & \\text{with dimensions 2x1; for this one, we would only consider the weights part of the W vector} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "And the derivatives with respect to the weights and biases of the 1st layer are:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial W^{(1)}} &= \\frac{\\partial E}{\\partial H^{(1)}}\\frac{\\partial H^{(1)}}{\\partial W^{(1)}} = X \\cdot \\delta^{(1)'} & \\text{with dimensions 4x2, so we can subtract right away}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And that's it!  \n",
    "As soon as we've calculated $\\delta^{(2)}$, we can update the weights of the 2nd layer as GD indicates. The weights for the 1st layer are updated at the end, since that is the final layer. If we had multiple layers, we can generalize the process: update the weights of layer $l+1$ after calculating the local gradient of layer $l$.  \n",
    "In case we feel like we have enough memory/storage, we could save the information about the gradients, but it's really not necessary. We'll see how the implementation goes once we get to it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing Gradient  \n",
    "\n",
    "We've been hearing a lot about gradients, and how they are used to update the weights. We've seen that we multiply lots of things together. The problem is with what kinds of numbers we multiply. In theory, we see mostly letters. The notations we use hide the fact that real applications tend to work with really small numbers. As layers are added, the backpropagation algorithm has us multiply lots of small numbers together. There comes a point at which the gradients are so small, they are practically 0. They \"vanish\", which is why we call this the \"*vanishing gradient problem*\".  \n",
    "\n",
    "To counterract this problem, we try to use activation functions that have a larger derivative. Guess what, we've been doing that all along! ReLU is one of the functions that allows us to keep our gradients as untouched as possible, since we generally multiply the numbers by 1, so we do nothing to them.  \n",
    "\n",
    "If you would look at the derivative of the sigmoid function, you would notice (even if by eye) that it is quite small. Numbers aren't necessary here, since we know for sure it's going to be quite smaller than 1. This is why we use ReLU, which has a derivative of 1 for all positive numbers. These choices are very important, since they affect the performance of a neural network with many hidden layers. As a trivia side note, once a network has more than 2 hidden layers, it is called a *deep neural network*.  \n",
    "\n",
    "These days, deep neural networks can have tens, if not hundreds or thousands of layers. That would be the new definition of \"deep\" today."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression vs Classification\n",
    "\n",
    "Up until this point, we've been talking about regression. Of course, neural networks can also classify data. You might be wondering about the modifications we would need to make to the network. Not much, actually. No matter the architecture, purpose, or other details, a neural network is meant to do calculations on numbers and output something we might find useful.  \n",
    "In the case of classification, the modifications we would need have to do with the activation function. Classification (especially when we have multiple output neurons) needs numbers that are between 0 and 1. When there are multiple percentages, they need to add to 1. A function that does this is the *softmax* function. Out of a list of numbers, it normalizez them so that they represent percentages of a whole, so they add to 1. In other word, it turns the outputs into probabilities of the input belonging to a certain class.  \n",
    "\n",
    "Another solution to the classifications problem is the *sigmoid* function, which is a special case of the softmax function. It is used when we have only 2 classes, since it maps any real number to the (0,1) interval. We've mentioned it as an activation function (but not popular due to the vanishing gradient problem), it can also be used as the output function, but it's not the most common choice since the softmax function is more general.  \n",
    "Yet another, more primitive, solution is a hard threshold. We do our calculations, and if the output is above a certain threshold, we classify it as belonging to one class, and if it's below, we classify it as belonging to the other class. This is a very rigid option, and you probably won't ever see it in practice outside teaching examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
