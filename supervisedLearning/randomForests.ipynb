{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "\n",
    "### Introduction\n",
    "As we've mentioned in the previous chapter (see [decision trees](decisionTrees.ipynb)), decision trees are often used, but not as the single, final, best method. You are far more likely to encounter the concept of decision trees in the context of **random forests**. Moreover, other techniques you *will* see being used together with DT and RF are *boosting* and *bagging*.  \n",
    "In a more general sense, these are all **ensemble methods**. Ensemble methods are methods that combine the predictions of several base estimators (say, some decision trees) in order to improve generalizability / robustness over a single estimator. We will talk about these methods in more detail in the next chapter. Random Forests themselves are a type of ensemble method, and we will talk about them in this chapter. We introduce ensemble methods through RF, then we will talk about them in more detail in the next chapter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a random forest?\n",
    "\n",
    "A random forest is a *collection of decision trees*, trained on different subsets of the data. The idea is that each tree in the forest is trained on a different subset of the data, and the predictions of each tree are combined to get the final prediction. The final prediction is the class that is most common among the predictions of the trees in the forest. The idea is to \"*listen*\" to the crowd, and take the majority vote.  \n",
    "\n",
    "As such, all we need to do is have 3 components at the ready:\n",
    "- A way to train a decision tree\n",
    "- A way to sample the data (to get different subsets of the data)\n",
    "- A way to combine the predictions of the trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Import sklearn iris\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from previous chapter\n",
    "# Gini impurity - it doesn't have to be complex because we only have 2 features; we really need to calculate only 1 probability, since the other one is 1 - p\n",
    "def gini(p):\n",
    "    #return 1 - p**2 - (1 - p)**2 \n",
    "    # do the math and the equation is equal to 2 * p * (1 - p)\n",
    "    return 2 * p * (1 - p)\n",
    "\n",
    "def gini_impurity(y):\n",
    "    p = np.sum(y == 0) / len(y)\n",
    "    return gini(p)\n",
    "\n",
    "# Recursive binary splitting\n",
    "def split(X, y, d, value):\n",
    "    # Split the data on feature 'd' at 'value'\n",
    "    left_indices = (X[:, d] <= value)\n",
    "    X_left, y_left = X[left_indices], y[left_indices]\n",
    "    X_right, y_right = X[~left_indices], y[~left_indices]\n",
    "    return X_left, X_right, y_left, y_right\n",
    "\n",
    "# Find the best split based on the mean value of each feature\n",
    "def find_best_split(X, y):\n",
    "    best_gini = 1.0\n",
    "    best_d, best_value = None, None\n",
    "    for d in range(X.shape[1]): # for each feature\n",
    "        value = np.mean(X[:, d]) # find the mean value\n",
    "        X_left, X_right, y_left, y_right = split(X, y, d, value) # split the data based on the mean of that feature\n",
    "        gini_left, gini_right = gini_impurity(y_left), gini_impurity(y_right) # calculate the gini impurity of the left and right splits\n",
    "        gini = (len(y_left) / len(y)) * gini_left + (len(y_right) / len(y)) * gini_right # calculate the weighted gini impurity\n",
    "        if gini < best_gini: # if the weighted gini impurity is less than the best gini impurity, update the best gini impurity (we have only 2 features, we could've calculated them directly but this is more general)\n",
    "            best_gini = gini\n",
    "            best_d = d # best feature index\n",
    "            best_value = value # best value for that feature (the mean in our case)\n",
    "    return best_d, best_value\n",
    "\n",
    "# Decision tree class\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=2, majThreshold = 0.8):\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "        self.majThreshold = majThreshold\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._grow_tree(X, y)\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        num_samples_per_class = [np.sum(y == i) for i in range(y.max() + 1)]\n",
    "        most_samples_per_class = np.max(num_samples_per_class)\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "        node = Node(predicted_class=predicted_class)\n",
    "        if depth < self.max_depth and most_samples_per_class / np.sum(num_samples_per_class) < self.majThreshold:\n",
    "            d, value = find_best_split(X, y)\n",
    "            if d is not None:\n",
    "                X_left, X_right, y_left, y_right = split(X, y, d, value)\n",
    "                node.d = d\n",
    "                node.value = value\n",
    "                node.left = self._grow_tree(X_left, y_left, depth + 1)\n",
    "                node.right = self._grow_tree(X_right, y_right, depth + 1)\n",
    "        return node\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.left is None and node.right is None:\n",
    "            return node.predicted_class\n",
    "        if x[node.d] <= node.value:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "    def accuracy(self, X, y):\n",
    "        return np.sum(self.predict(X) == y) / len(y)\n",
    "\n",
    "# Node class\n",
    "class Node:\n",
    "    def __init__(self, predicted_class):\n",
    "        self.predicted_class = predicted_class\n",
    "        self.d = None\n",
    "        self.value = None\n",
    "        self.left = None\n",
    "        self.right = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And use some new functions to create a random forest\n",
    "\n",
    "def trainDecisionTree(X, y, maxDepth=1, majThreshold = 0.8):\n",
    "    # Create a new decision tree\n",
    "    tree = DecisionTree(maxDepth, majThreshold)\n",
    "    # Train it\n",
    "    tree.fit(X, y)\n",
    "    # Return it\n",
    "    return tree\n",
    "\n",
    "def trainRandomForest(X, y, nTrees=10, maxDepth=1, majThreshold=0.7):\n",
    "    # Create a list of trees\n",
    "    trees = []\n",
    "    # For each tree\n",
    "    for i in range(nTrees):\n",
    "            # Train a decision tree\n",
    "            tree = trainDecisionTree(X[i], y[i], maxDepth, majThreshold)\n",
    "            # Add it to the list of trees\n",
    "            trees.append(tree)\n",
    "    # Return the list of trees\n",
    "    return trees\n",
    "\n",
    "def predictRandomForest(trees, X):\n",
    "    # Create a list of predictions\n",
    "    predictions = []\n",
    "    # For each tree\n",
    "    for tree in trees:\n",
    "            # Make a prediction\n",
    "            prediction = tree.predict(X)\n",
    "            # Add it to the list of predictions\n",
    "            predictions.append(prediction)\n",
    "    # Convert the list of predictions to a numpy array\n",
    "    predictions = np.array(predictions)\n",
    "    # Return the most common prediction for each sublist\n",
    "    return np.array([np.bincount(prediction).argmax() for prediction in predictions.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample our dataset, based on number of trees we want to create\n",
    "def sampleDataset(X, y, nSamples):\n",
    "    # Create a list of random indices\n",
    "    indices = np.random.choice(len(X), int(nSamples))\n",
    "    # Return the X and y samples\n",
    "    return X[indices], y[indices]\n",
    "# Create n random samples\n",
    "def createRandomSamples(X, y, noTrees, sampleSize):\n",
    "    # Create a list of samples\n",
    "    samplesX = []\n",
    "    samplesY = []\n",
    "    # For each sample\n",
    "    for i in range(noTrees):\n",
    "            # Create a random sample\n",
    "            sampleX, sampleY = sampleDataset(X, y, sampleSize)\n",
    "            # Add it to the list of samples\n",
    "            samplesX.append(sampleX)\n",
    "            samplesY.append(sampleY)\n",
    "    # Return the list of samples\n",
    "    return samplesX, samplesY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "# Sample our data to prepare for tree training\n",
    "noTrees = 10\n",
    "sampleSize = 50\n",
    "Xsampled, ySampled = createRandomSamples(X, y, noTrees, sampleSize)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we cannot control the seed of the random sequence generator we used above, so results will vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train noTrees trees as part of the random forest\n",
    "maxDepth = 2\n",
    "majThreshold = 0.8\n",
    "trees = trainRandomForest(Xsampled, ySampled, noTrees, maxDepth, majThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8133333333333334\n"
     ]
    }
   ],
   "source": [
    "# Let's see predictions now\n",
    "predictions = predictRandomForest(trees, X)\n",
    "# And the accuracy\n",
    "print(\"Accuracy: \", np.sum(predictions == y) / len(y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my runs, I see around 0.74 accuracy; not that big of an improvement.  \n",
    "\n",
    "This is with majThreshold = 0.8 (80% of the trees need to vote for the same class) & maxDepth = 2 & 10 trees & 50 sample size.\n",
    "\n",
    "Running with other values for the parameters might improve score, but not by much compared to the single decision tree with a bigger depth. So we need to improve in other regards.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "\n",
    "Up until this point, I just threw some code on the table without going into detail. We've seen that RF are simply a bunch of DTs, trained on various datasets. Of course we had to choose differents subsets of our whole dataset, since training everything on the same data would be pointless.  \n",
    "Let's look again at the sampling process:\n",
    "```\n",
    "def sampleDataset(X, y, nSamples):\n",
    "    # Create a list of random indices\n",
    "    indices = np.random.choice(len(X), nSamples)\n",
    "    # Return the X and y samples\n",
    "    return X[indices], y[indices]\n",
    "```\n",
    "This function just takes random samples from the dataset. It's not very sophisticated, but it works. Without explicitly telling it to, the function will take samples with replacement (default value of that parameter is **true**). This means that the same sample can be taken multiple times. This is not a problem, since we are not using the samples to train a single tree, but to train multiple trees. This is called **bootstrap sampling**. \n",
    "This is a very common way to sample data, and it is used in many other algorithms as well. As such, this is not a problem for us.  \n",
    "\n",
    "### Training\n",
    "\n",
    "Our training process assumes numerical values (which is true, in this case). We split the data until we run into our stopping criteria. We choose the classified class based on majority vote. These are concepts we've already discussed in the previous chapter, so now let's consider alternatives.  \n",
    "We can use other stopping criteria, such as the number of samples in a node, or the number of nodes in a tree. \"Number of samples in a node\" means that when a node has less than a certain number of samples, we stop splitting. \"Number of nodes in a tree\" means that when a tree has more than a certain number of nodes, we stop splitting.  \n",
    "We could think of other stopping criteria as well. For example, we could stop splitting when the entropy of a node is below a certain threshold, where entropy is defined as:\n",
    "$$\n",
    "H = -\\sum_{i=1}^n p_i \\log_2 p_i\n",
    "$$\n",
    "where $p_i$ is the probability of class $i$ in the node.  \n",
    "\n",
    "As you can see, there are many ways to stop splitting.  \n",
    "\n",
    "### Combining\n",
    "\n",
    "We've seen that we can combine the predictions of the trees in the forest by taking the majority vote (the most common value is also called the *mode*). This is a very simple way to combine the predictions. In the case of continuous values, we can use any of the *average, mean or median*.  \n",
    "We used the mode. Other options for categorical values are the *weighted mode* and the *weighted average*. These mean that we take the mode, but we weight the votes of each tree by the accuracy of that tree. This is a very simple way to improve the accuracy of the forest.  \n",
    "\n",
    "One thing we should not miss while we're here is that increasing the number of trees is quite likely to increase our accuracy. A word of caution, though: always be wary of overfitting. DTs are quite prone to overfitting, which is why we prefer to have multiple trees, all independently trained on different subsets of the data. Imagine asking 1000 people their opinion on a certain topic. The more people you ask (considering they don't influence each other), the more likely you are to get a good answer. This is the same idea.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've considered some of the decision points that make a random forest work, let's see how sklearn performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sklearn's random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=2, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=2, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=2, random_state=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use sklearn\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "print(\"Accuracy: \", clf.score(X, y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the accuracy is a lot higher (although it might be random, the difference should be around 0.2, so quite significant). Let's explore the default parameters to see how the library's solution differs from ours.  \n",
    "\n",
    "Here are some of the more important parameters:\n",
    "- n_estimators: the number of trees in the forest\n",
    "- max_depth: the maximum depth of the tree\n",
    "- criterion: the function to measure the quality of a split\n",
    "- max_features: the number of features to consider when looking for the best split\n",
    "- min_samples_split: the minimum number of samples required to split an internal node\n",
    "\n",
    "Looking at default values, the major difference is the number of trees. We used 10, while sklearn uses 100. This is quite a big step-up. We set the max_depth to 2, and both criteria use \"gini\" (which is the default). The minimum number of samples to split is 2, which is the default. One difference then is that we we adopted the idea of \"majority vote\", while sklearn makes sure first that there is a minimum of 2 samples in each node. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# RF with our code using 100 trees\n",
    "Xsampled, ySampled = createRandomSamples(X, y, 100, 50)\n",
    "trees = trainRandomForest(Xsampled, ySampled, 100, maxDepth = 2, majThreshold = 0.5)\n",
    "predictions = predictRandomForest(trees, X)\n",
    "print(\"Accuracy: \", np.sum(predictions == y) / len(y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, it seems that increasing trees doesn't work that well. Increasing majThreshold does instead increase our accuracy. This means our point of failure lies within the DT itself, namely in the way it's built. So let's explore that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning & Improving\n",
    "\n",
    "Although we've left the Decision Tree chapter without giving much thought to improving performance, now is a good chance to explore weak points in our method and improve on them. In a real-world scenario, we would just use a framework's implementation (unless we're doing research, of course). However, we're still in the learning phase, so my goal is to work the **let's make it better** mindset. \n",
    "\n",
    "\n",
    "\n",
    "In an RF, decision trees are supposed to be as different as possible from each other. This is why we use different subsets of the data to train them. However, we're still using the same features to train them. This means that they are still quite similar. We can improve this by using a different set of features for each tree. This is called **feature bagging**. We'll create new functions that allow for this extra option. We'll then use them to train a new forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New function to train DTs that takes a sample of features\n",
    "def trainDecisionTree(X, y, maxDepth=1, majThreshold = 0.8, nFeatures = 2):\n",
    "    # Create a new decision tree\n",
    "    tree = DecisionTree(maxDepth, majThreshold)\n",
    "    # Train it\n",
    "    tree.fit(X, y, nFeatures)\n",
    "    # Return it\n",
    "    return tree\n",
    "# New function to find best split of a feature, based on a sample of features\n",
    "def find_best_split(X, y, features):\n",
    "    best_gini = 1.0\n",
    "    best_d, best_value = None, None\n",
    "    for d in features: # for each feature\n",
    "        value = np.mean(X[:, d]) # find the mean value\n",
    "        X_left, X_right, y_left, y_right = split(X, y, d, value) # split the data based on the mean of that feature\n",
    "        gini_left, gini_right = gini_impurity(y_left), gini_impurity(y_right) # calculate the gini impurity of the left and right splits\n",
    "        gini = (len(y_left) / len(y)) * gini_left + (len(y_right) / len(y)) * gini_right # calculate the weighted gini impurity\n",
    "        if gini < best_gini: # if the weighted gini impurity is less than the best gini impurity, update the best gini impurity (we have only 2 features, we could've calculated them directly but this is more general)\n",
    "            best_gini = gini\n",
    "            best_d = d # best feature index\n",
    "            best_value = value # best value for that feature (the mean in our case)\n",
    "    return best_d, best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New DT class that uses max features parameter to train\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=2, minSamples = 2, maxFeatures = 1):\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "        self.minSamples = minSamples\n",
    "        self.maxFeatures = maxFeatures\n",
    "    def fit(self, X, y, maxFeatures):\n",
    "        self.root = self._grow_tree(X, y)\n",
    "    def _grow_tree(self, X, y, depth=0, maxFeatures = 1):\n",
    "        num_samples_per_class = [np.sum(y == i) for i in range(y.max() + 1)]\n",
    "        most_samples_per_class = np.max(num_samples_per_class)\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "        node = Node(predicted_class=predicted_class)\n",
    "        featureSampled = np.random.choice(X.shape[1], maxFeatures, replace=False)\n",
    "        if depth < self.max_depth and most_samples_per_class > self.minSamples:\n",
    "            d, value = find_best_split(X, y, featureSampled)\n",
    "            if d is not None:\n",
    "                X_left, X_right, y_left, y_right = split(X, y, d, value)\n",
    "                node.d = d\n",
    "                node.value = value\n",
    "                node.left = self._grow_tree(X_left, y_left, depth + 1)\n",
    "                node.right = self._grow_tree(X_right, y_right, depth + 1)\n",
    "        return node\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.left is None and node.right is None:\n",
    "            return node.predicted_class\n",
    "        if x[node.d] <= node.value:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "    def accuracy(self, X, y):\n",
    "        return np.sum(self.predict(X) == y) / len(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can expand the code above to analyze it, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new RF class that uses max features parameter to train\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees=10, max_depth=2, minSamples = 2, maxFeatures = 1):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.minSamples = minSamples\n",
    "        self.maxFeatures = maxFeatures\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        for _i in range(self.n_trees):\n",
    "            tree = DecisionTree(self.max_depth, self.minSamples, self.maxFeatures)\n",
    "            tree.fit(X[_i], y[_i], self.maxFeatures)\n",
    "            self.trees.append(tree)\n",
    "    def predict(self, X):\n",
    "        y_preds = np.empty([self.n_trees, len(X)])\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            y_preds[i] = tree.predict(X)\n",
    "        y_pred = []\n",
    "        for sample_predictions in y_preds.T:\n",
    "            y_pred.append(np.bincount(sample_predictions.astype('int')).argmax())\n",
    "        return np.array(y_pred)\n",
    "    def accuracy(self, X, y):\n",
    "        return np.sum(self.predict(X) == y) / len(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RF with 100 trees, max depth 2, min samples 2 and max features 2\n",
    "rf = RandomForest(n_trees=100, max_depth=2, minSamples = 2, maxFeatures = 2)\n",
    "Xsampled, ySampled = createRandomSamples(X, y, 100, 30)\n",
    "rf.fit(Xsampled, ySampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9266666666666666\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of rf\n",
    "print(\"Accuracy: \", rf.accuracy(X, y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go! Although not quite as good as the sklearn implementation, we've managed to improve our accuracy quite a bit. Remember, results vary (in my case, I went from 0.74 to around 0.9) due to the random nature of the sampling process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember\n",
    "\n",
    "What I want you to remember from this chapter is that a Random Forest is only as good as it's trees. They have to be as different as possible from each other, so they come together to form one objective answer. Remember the 1000 people example. When we presented that idea earlier, we mentioned that those people should **not** influence each other. The way we would implement this within our own RF algorithms is by making sure that trees get different conditions to train on. Of course, we could go even further and look at other options to create more diversity in the trees. But I'll leave that option as an exercise for the reader."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next chapter: [Ensemble Methods](ensembleMethods.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commonMLalgs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Oct 13 2022, 21:15:33) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "500cef33aa921c87c63c1753d003ac956bc931603b9c07a450ae237ef80e36a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
