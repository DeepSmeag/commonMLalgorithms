{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook, we will discuss the bias-variance tradeoff and overfitting. We will also discuss some strategies depending on problems that arise when fitting a model.\n",
    "\n",
    "# Bias\n",
    "This term defines the \"rigidity\" of a model. A model with high bias makes strong assumptions about what the data looks like. Therefore, if those assumptions are not true, the model will not fit the data well. A model with high bias disregards patterns in the data. We say that it is underfitting.\n",
    "### Examples\n",
    "* Linear regression is a model with high bias. It assumes that the data is linear and that the relationship between the features and the target is linear. If the data is not linear, the model will not fit the data well.\n",
    "* Decision trees are a model with low bias. They can fit a wide range of data. However, they are very flexible and can overfit the data.\n",
    "\n",
    "# Variance\n",
    "This term defines the \"sensitivity\" of a model. A model with high variance is very sensitive to the training data. Therefore, if the training data is noisy, the model will not generalize well. A model with low variance is less sensitive to the training data. However, this means that the model will not fit the data as well.  \n",
    "We could also think of it as \"flexibility\". A high-variance model is very flexible and can fit a wide range of data. However, this flexibility comes at the cost of overfitting.\n",
    "### Examples\n",
    "* Linear regression is a model with low variance. It is not very flexible and will not fit a wide range of data. However, it will not overfit the data.\n",
    "* Decision trees are a model with high variance. They are very flexible and can fit a wide range of data. However, they can overfit the data.\n",
    "\n",
    "# Bias-variance tradeoff\n",
    "As you've probably noticed from the examples, when we say that a model has high bias, we also say that it has low variance. So these 2 terms are like opposites.  \n",
    "**They have to be balanced.** Ideally, a model should have both low bias and low variance, but this is not always possible. In fact, most of the time it's not possible.  \n",
    "\n",
    "This is why we often have to choose between them. It's a decision you have to make whether you want to add more bias to your model to reduce the variance or add more variance to your model to reduce the bias. How you do that specifically depends on the model you're using. \n",
    "\n",
    "## Strategies\n",
    "One common term you'll find regarding overfitting is **regularization**. This method involved adding some kind of punishment to the cost function when overfitting occurs. As such, the model becomes more biased.  \n",
    "Another common strategy is to increase or decrease the complexity of the model. This is a direct tweak to influence the bias and/or variance. For example, increasing the complexity of a linear model in linear regression would mean going the polynomial route, with higher-order degrees.\n",
    "Other strategies include *dropout layers* (when talking about neural networks), *cross-validation*, *early stopping*, etc.  \n",
    "You could even add some random events that make the model test out new parameter values from time to time. This method is very dependent on the specifics of your model and the problem you're trying to solve, but introducing randomness can help reduce overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
