{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Mathematics behind Support Vector Machines\n",
    "\n",
    "It's a big topic, so we will cover the essentials. A lot of the math is skipped, but resources that explain everything fully are given along the way. We'll start with the prerequisites:  \n",
    "\n",
    "* Linear Algebra\n",
    "* Calculus  \n",
    "\n",
    "You would need these at a college-level understanding.\n",
    "\n",
    "You should understand these concepts:\n",
    "- Vectors & Matrices\n",
    "- Derivatives + partial derivatives\n",
    "- Distances in n-dimensional space\n",
    "- Function optimization with/without constraints (Lagrange multipliers) - *some resources will be provided*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the basic idea of SVM classifiers is the maximize the margin (distance from the hyperplane to the nearest data points) between the two classes. Let's think about how this is calculated.  \n",
    "\n",
    "Let's say we have a hyperplane. We calculate the distance to the closest data point (because that datapoint does in fact represent a sort of barrier we cannot cross - at least while we're talking about hard-margin classifiers). If we double that distance, we have our margin.  \n",
    "Of course, we want to maximize our margin, which means that our goal is to find a middle ground between the two classes (which leads to the maximum possible margin).  \n",
    "To do that, we introduce a few ideas:  \n",
    "- each point in n-space is essentially a vector.\n",
    "- our hyperplane is an (n-1)-dimensional space, which means that it is a vector in n-space (because we add back the intercept-term).  \n",
    "\n",
    "So we have 2 vectors: a point A, and our normal vector W (which describes the hyper-plane; remember that this vector will always be normal to the actual hyperplane representation in a graph). To get the distance between the hyperplane and the point A, we have to *project* A onto W, then calculate the norm of that projection. Double that, and we have our margin calculated. The steps are relatively simple, and the math is easy to apply, once we get the hang of it.  \n",
    "\n",
    "Formula for projection of A onto W (let **U** be the unit vector of W, *aka* the direction of W; let **P** be the projection):\n",
    "$$\n",
    "U = \\frac{W}{||W||}\n",
    "$$\n",
    "$$\n",
    "||P|| = U \\cdot A\n",
    "$$  \n",
    "$$\n",
    "P = (U \\cdot A) \\cdot U\n",
    "$$\n",
    "\n",
    "\n",
    "Formula for norm:\n",
    "$$\n",
    "\\| \\vec{A} \\| = \\sqrt{\\vec{A} \\cdot \\vec{A}}\n",
    "$$\n",
    "\n",
    "Note that we would only be interested in the norm of the projection, not the projection itself. We would simply calculate $||P|| = U \\cdot A$, and double it (because this value is only one side of the hyperplane, and we want to take into account both sides). That would be our margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know how to calculate the margin for one hyperplane. Let's see how we can find the optimal hyperplane, with the biggest margin.  \n",
    "\n",
    "Since our goal is to find the biggest value for the margin, this means we find the biggest norm for $P$, our projection vector. As we now know, $||P|| = U \\cdot A$.  \n",
    "$U = \\frac{W}{||W||}$, so we can substitute that into the equation for $||P||$. \n",
    "$$\n",
    "||P|| = \\frac{W}{||W||} \\cdot A\n",
    "$$\n",
    "$$\n",
    "||P|| = \\frac{W \\cdot A}{||W||}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The math does involve some more steps, but I'm going to simplify it a bit.  \n",
    "If you want to see each step of the way, I would recommend checking out this [article](https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-2/). You might want to read every part, it's worth it.  \n",
    "\n",
    "All right, the simplification is as follows: to get $||P||$ as big as possible, we look at our variables. We have $W$, that's our hyperplane vector. We change this, it's norm changes. We have a few ways in which we could change it, but which one is the best? Let's go to $A$. This is a datapoint, so we know for sure we're not modifying anything about that. Lastly, we have $||W||$, which depends on $W$. What's different now is that instead of having so many directions in which we could change $W$, we only have 1 for $||W||$: make it smaller. To maximize $||P||$, we want to make $||W||$ as small as possible. And that gives us a target:  \n",
    "\n",
    "#### Minimize the norm of our weights vector W\n",
    "\n",
    "I will skip the math in the middle, but eventually equations lead to this:\n",
    "\n",
    "$$\n",
    "m = \\frac{2}{||W||}\n",
    "$$\n",
    "\n",
    "If we were looking at the full mathematics, we would eventually reach this result. So now we have a target. This is an optimization problem. Since we have no other conditions, we call this a **unconstrained optimization problem**. Things are a bit more complicated going forward.  \n",
    "In order to understand things going forward, derivatives + lagrange multipliers are required (at least a basic understanding).  \n",
    "Here is a link to a playlist [about lagrange multipliers](https://www.youtube.com/playlist?list=PLCg2-CTYVrQvNGLbd-FN70UxWZSeKP4wV) that I found helpful.\n",
    "Derivatives (+gradients) would be covered in a calculus course, so here is [another playlist covering many aspects of calculus](https://www.youtube.com/watch?v=WUvTyaaNkzM&list=PL0-GT3co4r2wlh6UHTUeQsrf3mlS2lk6x).\n",
    "It's hard to understand everything without a lot of practice, so more work outside of this notebook is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until this point, we've focused on the aspect of maximizing the margin. The next point is to make sure that we don't cross the margin. This is where the constraints come in. Remember those dotted lines we plotted along with the hyperplane? Those are (essentially) constraints. We want to make sure that points within those dotted lines are not misclassified. The mathematical way of saying that is:  \n",
    "\n",
    "Given a hyperplane with parameters W that adheres to the following equation:\n",
    "$$\n",
    "W \\cdot X + b = 0\n",
    "$$\n",
    "This means that every point $X$ on the hyperplane satisfies this equation. Other points (such us from our dataset) will be $>0$ or $<0$, depending on their respective class. We want to make sure that the points from our dataset are on the correct side of the hyperplane.\n",
    "\n",
    "We can write equations for 2 other hyperplanes that are equidistant from our main hyperplane:\n",
    "$$\n",
    "W \\cdot X + b = δ\n",
    "$$\n",
    "$$\n",
    "W \\cdot X + b = -δ\n",
    "$$\n",
    "where δ is the distance from the main hyperplane to the dotted lines (so half our margin).\n",
    "\n",
    "In practice, we set $δ = 1$, but we can set it to any value we want. This is to simplify calculations. Ultimately, what we set this value to doesn't matter, and we'll see why shortly.  \n",
    "Our 2 hyperplanes equations now look like this:\n",
    "$$\n",
    "W \\cdot X + b = 1\n",
    "$$\n",
    "$$\n",
    "W \\cdot X + b = -1\n",
    "$$\n",
    "We want all of the points from our dataset to be on the correct side of these hyperplanes. This means that we want all of the points from our dataset to satisfy the following equations:\n",
    "$$\n",
    "W \\cdot X + b \\geq 1, \\text{ if } y = 1\n",
    "$$\n",
    "$$\n",
    "W \\cdot X + b \\leq -1, \\text{ if } y = -1\n",
    "$$\n",
    "Where $y$ is the correct class. We can rewrite those 2 equations as:\n",
    "$$\n",
    "y(W \\cdot X + b) \\geq 1\n",
    "$$\n",
    "Which then becomes:\n",
    "$$\n",
    "y(W \\cdot X + b) - 1 \\geq 0\n",
    "$$\n",
    "And this is our constraint. We want to make sure that this constraint is satisfied for all of our datapoints.  \n",
    "\n",
    "Let's take a step back. We now have an optimization problem with constraints. Our problem looks like this:\n",
    "$$\n",
    "\\underset{W}{\\text{minimize}} \\frac{1}{2} ||W||^2\n",
    "$$\n",
    "$$\n",
    "\\text{subject to} \\quad y(W \\cdot X + b) - 1 \\geq 0\n",
    "$$\n",
    "We can now use the lagrange multiplier method to solve this problem. And that's outside the scope of this notebook. Refer to videos above and [this article](https://www.svm-tutorial.com/2016/09/unconstrained-minimization/) for more information.  \n",
    "\n",
    "What happens is that we consider the following functions:\n",
    "$$\n",
    "f(W) = \\frac{1}{2} ||W||^2 \\\\\n",
    "g(W,b) = y(W \\cdot X + b) - 1\n",
    "$$\n",
    "\n",
    "--------------\n",
    "#### NOTE\n",
    "In the case of soft-margin classifiers, we allow some room for error. That is where the $C$ parameter comes in. Formulas change a bit. Our $f$ function would look like this:\n",
    "$$\n",
    "f(W) = \\frac{1}{2}||W||^2 + C \\sum_{i=1}^{n} \\xi_i\n",
    "$$\n",
    "Where $\\xi_i$ is the error for the $i^{th}$ datapoint.\n",
    "\n",
    "-----------\n",
    "\n",
    "We then consider their gradients, and have the following condition:\n",
    "$$\n",
    "\\nabla f(W) - \\lambda \\nabla g(W,b) = 0\n",
    "$$\n",
    "Where $\\lambda$ is the lagrange multiplier. This means we want the gradient of $f(W)$ to be equal to the gradient of $g(W,b)$, multiplied by the lagrange multiplier. Note that + or - means the same thing here, the multiplier can absorb the sign. You will also see people writing the equation with a + sign, but it's the same thing.  \n",
    "\n",
    "The method then defines a new function:\n",
    "$$\n",
    "L(W,b,\\lambda) = f(W) - \\lambda g(W,b)\n",
    "$$\n",
    "And takes its gradient:\n",
    "$$\n",
    "\\nabla L(W,b,\\lambda) = \\nabla f(W) - \\lambda \\nabla g(W,b)\n",
    "$$\n",
    "If we set it to 0, we find points where the gradients of $f$ and $g$ are paralell (meaning a lot of things depending how you interpret them). What matters is that solving this last equation gives us the results we want.  \n",
    "\n",
    "Again, a lot of math has been skipped, and I don't even claim to have done a good job with these last few paragraphs. I just wanted to give you an idea of what steps are involved, so you know what you can search for. Going forward in this course, a lot more math will be skipped, claiming previous knowledge (or just focusing on what I believe matters more). Instead of being able to spew the equations from memory, I want you to know how to explain the concepts at a high level, and eventually how to find equations (either by yourself or by searching on the internet), if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What comes next\n",
    "\n",
    "Next, we'll be looking at Decision Trees. They are a very popular algorithm, when combined with some other techniques we will also be talking about. You can find the notebook here: [Decision Trees](decisionTrees.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10 (default, Mar 15 2022, 12:22:08) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
