{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "\n",
    "In this chapter, we will discuss ensemble methods in-depth: why they exists, what they have to offer and where to use them.  \n",
    "Ensemble methods are a type of machine learning algorithm that combines multiple base models (we call them *\"weak learners\"*) in order to produce one optimal predictive model (*\"strong learners\"*). Ensemble methods are very popular in machine learning competitions because they often produce the best predictive models. In this chapter, we will cover the following subjects:\n",
    "- Bagging\n",
    "- Boosting\n",
    "- Stacking  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Ensemble Methods?\n",
    "\n",
    "As we've mentioned above, ensemble methods often produce the best results. Let's go back to our *1000 people* example. We know through common sense that askins a lot of people for their opinion on a topic will often lead to a better result than asking only one person. But why?  \n",
    "\n",
    "There are a few reasons. Of course, we're not citing research right now, but think about it: different people means different skillsets, different background, different experiences, different viewpoints. By attacking a problem from so many different angles, we're bound to reach a results that's close to the truth (or at least closer than if we only had one person's opinion).  \n",
    "\n",
    "This variety is what gives ensemble methods their power. We combine the opinions of many different models to reach a more accurate, more consistent result."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we didn't use multiple models?\n",
    "\n",
    "Well, we've already seen what happens. When forced to resort to a single model, we have to make multiple decisions. These are failure points. Each decision is something that requires a human to make a correct prediction about the best direction the model should take.  \n",
    "\n",
    "Things like data availability or cleanliness (think clean = organized, tabular data; dirty = random words on a page), feature engineering (choosing which parts of the data should be used where), model selection (thinking about and choosing a model that can describe the data accurately) and hyperparameter tuning (experimenting with different values and picking the best combination) are all decisions that can lead to failure.  \n",
    "\n",
    "In the end, we have no choice but to make decisions ourselves about those variables, and we're bound to make mistakes. Those mistakes add up. That's why ensemble methods come in handy: they allow us to average out the mistakes made by each model, so that the overall result is more accurate than any single model could have produced.  \n",
    "\n",
    "Another way to think about it: we've discussed the [bias-variance tradeoff](../otherConcepts/bias%26variance.ipynb). Choosing one single model forces us to have this tradeoff. Averaging across multiple models would allow us to keep a better balance between bias and variance, without explicitly having to search for and choose parameters and whatnot."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What they have to offer?\n",
    "\n",
    "We've already covered some of the benefits of these methods. Essentially,what we get is a more robust model that was built somewhat automatically (Note: really depends on how you build your ensemble). Besides this, we avoid the curse of having to choose a model's characteristics + parameters (like choosing degree of a polynomial model in a regression problem), since combining multiple models (that fit to multiple features) often generates a high-dimension fit. In other words, multiple simple models get combined into a complex model that can fit data points with a lot of dimensions (even highly non-linear distributions in some cases).  \n",
    "\n",
    "### !!\n",
    "\n",
    "<span style=\"color:red\">It's quite important</span> not to couple *the idea of ensemble methods* with the idea of a model. Think of this approach as a meta-model, since it can be applied to pretty much any kind of algorithm we can find out there. Indeed, most commonly we find ensemble methods applied to decision trees, but it's not a requirement. Depends on the problem, our choices, our ideas, our experience and many more factors like these."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where to use them?\n",
    "\n",
    "Ensemble methods are especially popular in machine learning competitions. This is because they allow for great results without much hassle. Of course, they are not limited to competitions. Any situation where we would have to deal with a lot of hyper-parameters, non-linear data points, or a big dataset would be a good fit for ensemble methods.  \n",
    "As a rule of thumb, if you're not sure what to do, try ensemble methods. They're a good starting point. If you see a problem and instantly think \"Ah, yes. I'll use <insert method name>\", then you probably know why you're doing it so maybe ensemble methods won't be the best first choice (although in general they seem to outperform simplistic models)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "\n",
    "Bagging stands for *bootstrap aggregating*. It's a method that combines multiple models to produce a more accurate result. The idea is to train multiple models on different subsets of the data, and then average their results. Think of it as a way to average out the mistakes made by each model.  \n",
    "\n",
    "Let's consider our previous Random Forest. What we did was train multiple decision trees with random subsets of the data and random subsets of the features. Then we combined them all together to get final result.  \n",
    "\n",
    "Picking out random subsets of the data is called *bootstrapping*. We do this because we want to avoid overfitting. If we trained all our models on the same data, they would all be the same. We want to avoid this. \"Bootstrapping\" is a term that comes from statistics, and it refers to the process of sampling data with replacement (You pick $X$ samples randomly, but you don't discard them after picking them out; each sample has the chance of being picked any number of times).  \n",
    "\n",
    "In our case, the RF process used boostrapping to create different subsets of the data (and features). Then, it used each subset to train a decision tree. Finally, it combined (aggregated) the results of all the trees to get a final result. That's bagging.  \n",
    "\n",
    "### Advantages\n",
    "\n",
    "- It's a very simple method that can be applied to any kind of model.\n",
    "- It can be used to reduce overfitting (increase bias).\n",
    "- (!!!)Models can be trained in parallel; this is a big one, since we make use of modern hardware efficiently; This means that bagging is a fast & scalable method.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- Limited by the quality of it's components; bagging relies on the overall accuracy of the many models it combines; When faced with a really complex problem with high-dimensional data, a single, more specialized model might be a better fit.\n",
    "- It's designed to average results, so to increase bias; as an addendum to the previous point, if we need high variance, bagging might not be the best choice.\n",
    "- When data is insufficient, trying to sample it multiple times would probably just lead to repeated samples; it would probably not lead to a much better solution compared to one single model that takes all the dataset as input.\n",
    "- It's both flexible and rigid. Since you rely on random choices, there is a chance you will simply be unlucky (just like there are chances to get great results). To improve the chances of a good result, we would increase the number of models (thus increasing diversity) + make sure to properly sample the data (avoiding sampling bias); One other important factor (although this applies to **any kind of model**): trash data, trash results. Collecting & organizing data is even more important than picking out the right model.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final note on Bagging\n",
    "\n",
    "Although we've talked about \"multiple models\", it's important to mention that the vision of bagging mainly relies on multiple models *of the same type*. In our case, we had the example of fitting multiple DTs. They are multiple models, but the same kind of model.  \n",
    "\n",
    "If we require different types of models, we get away from the idea of bootstrapping, which means we're not really doing bagging anymore. Instead, depending on the case, it would probably be classified as a combination of both bagging and stacking (more on that later)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "This family of methods works the same way as bagging: we combine multiple models to create a strong learner that performs better. The difference is that boosting is an iterative process. We start with a single model, and then we add more models to it. Each new model is trained on the data that the previous models failed to fit.  \n",
    "\n",
    "Let's put it another way: whereas bagging allows us to create many models in parallel (all at the same time, since they do not depend on each other in any way), boosting relies on continuously fitting weak learners to pieces of data that the previous models failed to fit.  \n",
    "\n",
    "This generally results in reducing bias (increasing variance).  \n",
    "To sum up: *Boosting is the process of iteratively fitting multiple models (weak learners) on observations (data) which the previous attempts failed to fit correctly (or had trouble doing so either way). We can use it for both regression and classification.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our main goal is to reduce bias, the models considered for boosting are usually ones with high bias and low variance. If we think about DTs, we would choose shallow trees, since they do not allow for overfitting to take place.  \n",
    "Another point of view is that we choose such models because we cannot train in parallel, therefore we need to make sure that each training process is as quick as possible. As such, simple models are preferred.  \n",
    "\n",
    "We are now faced with a few questions:  \n",
    "- how do we know which observations to focus on?\n",
    "- how do we combine our models?\n",
    "\n",
    "We'll discuss 2 methods: AdaBoost and Gradient Boosting. We'll see how they differ in their approaches and what these differences mean for the final result.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost = Adaptive Boosting\n",
    "\n",
    "Simply put, this method attempts to find a set of weights that are applied to each of the weak learners, so that the final result is as desired.  \n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Final result} &= \\sum_{i=1}^{n} \\alpha_i \\cdot \\text{Weak learner}_i \\\\\n",
    "\\text{where } \\alpha_i &\\text{ is the weight of the } i^{th} \\text{ weak learner}\n",
    "\\end{align}\n",
    "$$  \n",
    "\n",
    "How do we find these weights then? This is a tough optimization problem. What works better (although it does not necessarily guarantee the *best* result) is to iteratively fit weak learners to the data so that we find our weights step by step. Here's how it works:\n",
    "\n",
    "We define our models such that they are dependent on the previous models:\n",
    "$$\n",
    "\\begin{align}\n",
    "s_l(x) = s_{l-1}(x) + c_l \\cdot w_l(x)\n",
    "\\end{align}\n",
    "$$\n",
    "where $s_l(x)$ is the result of the $l^{th}$ model, $s_{l-1}(x)$ is the result of the previous model, $c_l$ is the weight of the $l^{th}$ model, and $w_l(x)$ is the result of the $l^{th}$ weak learner.  \n",
    "Models with $s$ are the strong learners, and models with $w$ are the weak learners. We define our result as the strong learner up to the $l^{th}$ model.  \n",
    "\n",
    "We choose $w$ and $c$ such that $s_l$ is the model that best fits the data.  \n",
    "What does \"best fits the data\" mean? Of course, we turn to the loss function. We want an error score that we can minimize to find the best model.  \n",
    "\n",
    "As such, our $w$ and $c$ are parameters chosen such that the loss function is minimized. Mathematically, we write:\n",
    "$$\n",
    "\\begin{align}\n",
    "(c_l, w_l) &= \\arg \\min_{c, w} \\mathcal{L}(s_{l-1}(x) + c \\cdot w(x), y) \\\\\n",
    "&= \\arg \\min_{c,w} \\sum_{n=1}^{N} \\mathcal{L}(s_{l-1}(x_n) + c \\cdot w(x_n), y_n) \\\\\n",
    "\\text{where } \\mathcal{L} &\\text{ is the loss function}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This process allows us to optimize \"locally\", instead of trying to directly find the best combination of models for the whole dataset.  \n",
    "**To sum up**: each weak learner is trained on some data; we associate a weight to it; we then add the result to the previous strong learner in order to create a new one. We repeat this process until we reach a desired number of models.  \n",
    "\n",
    "The training process for the weak learner is separated from the AdaBoost method. We assume that $w_l(x)$ is a known function, and we only need to find the best $c_l$ for each model. We do that by minimizing the loss function (through any method we want; gradient descent is an example)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving on\n",
    "\n",
    "Up until this point, we've discussed the 2nd question we had: how do we combine our models? We've come up with some sort of formula that allows us to take into account each of the previous models. What's left is to answer the first question: how do we know which observations to focus on?  \n",
    "\n",
    "Here is the important aspect: AdaBoost relies on repeatedly sampling our dataset to focus more and more on misclassified/mispredicted observations. This is done by assigning a weight to each observation, and then sampling the dataset according to these weights.  \n",
    "\n",
    "Let's look at the steps:\n",
    "1. Assign equal weights to all observations; if N is the number of observations, then each observation has a weight of $\\frac{1}{N}$.\n",
    "2. Fit a weak learner to the dataset, and calculate the error score.\n",
    "3. Calculate the weight of the weak learner. In the case of classification, we might use:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha_l = \\frac{1}{2} \\log \\left( \\frac{1 - \\epsilon_l}{\\epsilon_l} \\right)\n",
    "\\end{align} \\\\\n",
    "\\text{where $\\epsilon_l$ is the error score of the $l^{th}$ weak learner.}\n",
    "$$\n",
    "4. Update the weights of the observations. We want the weights of misclassified or mispredicted observations to increase, and the weights of correctly classified/predicted observations to decrease. Keep in mind that the sum of all weights should be equal to 1, so we need to normalize the weights after updating them. In the case of classification, we might use:\n",
    "$$\n",
    "\\begin{align}\n",
    "w_{n,l+1} = \\frac{w_{n,l} \\cdot \\exp \\left( - \\alpha_l \\cdot y_n \\cdot w_l(x_n) \\right)}{Z_l}\n",
    "\\end{align} \\\\\n",
    "\\text{where $w_{n,l}$ is the weight of the $n^{th}$ observation at the $l^{th}$ iteration,} \\\\\n",
    "\\text{\n",
    "$y_n$ is the true label of the $n^{th}$ observation,} \\\\\n",
    "\\text{\n",
    "$w_l(x_n)$ is the prediction of the $l^{th}$ weak learner for the $n^{th}$ observation, } \\\\\n",
    "\\text{\n",
    "and $Z_l$ is a normalization factor.  } \\\\\n",
    "\\text{This formula is based on having $exp(\\psi) > 1$ if the prediction is correct, } \\\\\n",
    "\\text{  \n",
    "and $exp(\\psi) < 1$ if the prediction is incorrect, which is why we use $y_n \\cdot w_l(x_n)$.}\n",
    "$$\n",
    "5. Recreate the dataset by sampling observations according to their weights. Over time, wrong predictions will be sampled more often, simulating the idea of focusing on the observations that are harder to predict.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking back\n",
    "\n",
    "We've now answered the 2 important questions. By combining weak learners based on their \"importance\" (weight), and by repeatedly focusing on the observations that are harder to predict, we can create a strong learner that is more accurate than any of the weak learners.\n",
    "\n",
    "Some notes:\n",
    "- AdaBoost is generally built on shallow decision trees, called decision stumps (depth = 1). This is because decision stumps are very fast to train, and they are easy to combine.\n",
    "- since the method relies on iteratively finding our best model through weak learners, we cannot parallelize the training process. Instead of trying to combine 1000s of weak learners through AdaBoost, we might be better off using a single strong learner with low bias (e.g. a neural network / high-degree polynomial model).\n",
    "- since we are applying a *greedy method* that focuses on finding the best result for every iteration, results are not guaranteed to be the best; we rely on the fact that they are good enough.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "Gradient Boosting works in a similar fashion to AdaBoost, in that they both try to combine weak learrners by summing a weighted series of models. The way they do it is different, however.  \n",
    "\n",
    "Let's first consider what AdaBoost does: at any point, we train a weak learner on the data set we have at hand, after which we add that weak learner to the previous strong learner, update the weights of the observations and resample the dataset.  \n",
    "\n",
    "Gradient Boosting works by using the previous strong learner to calculate the residuals (difference between the prediction and the true value). We then train a weak learner on the residuals, and add **a part** (the previous $c_l$ coefficient is now a *step size*) that model's insights to the previous strong learner. We'll dive deeper in a moment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent and Gradient Boosting\n",
    "\n",
    "We often learn better when anchoring our understanding of a new subject to something we already know.  \n",
    "We've learned about gradient descent, a method which allows us to find the parameters which minimize a certain function.  \n",
    "\n",
    "Gradient Boosting works similarly, in that we continuously train weak learners to minimize errors left by a previous strong learner. We call these errors \"residuals\". Like in GD, we use a learning rate parameter to control how much we add to the previous strong learner (varying from 0 to 1).\n",
    "\n",
    "Please pay attention to the following image. On the left we have weak learners that fit to their corresponding \"residuals\", and on the right we have the strong learner that is the sum of all the weak learners up to that point. As we iterate, we add more weak learners and the overal fit becomes better and better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image depicting strong learner progress and corresponding residuals; credit to Medium writer Rick Strahl](https://miro.medium.com/max/828/1*KmFFQVKyDPAD5Qu66hHbxg.webp)  \n",
    "Credit: Rick Strahl via [Medium](https://towardsdatascience.com/all-you-need-to-know-about-gradient-boosting-algorithm-part-1-regression-2520a34a502)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lesson we take from this is that repeatedly revising our errors allows us to reach a better result in the end (a small life lesson as a bonus).  \n",
    "\n",
    "Let's look at the mathematically equivalent formula for the strong learner and the residuals.\n",
    "\n",
    "$$\n",
    "\\hat{y}(x) = \\sum_{l=1}^L \\gamma_l \\cdot w_l(x) \\\\\n",
    "\\text{where $\\hat{y}(x)$ is the prediction of the strong learner,} \\\\\n",
    "\\text{$\\gamma_l$ is the weight of the $l^{th}$ weak learner,} \\\\\n",
    "\\text{and $w_l(x)$ is the prediction of the $l^{th}$ weak learner.}\n",
    "$$\n",
    "\n",
    "As with AdaBoost, the method essentially finds a weighted sum that leads to the best result. What we care about is the $w_l(x)$, which is the prediction of the $l^{th}$ weak learner. Imagine we create a new dataset that only contains the errors of the previous strong learner. It would look like this:\n",
    "$$\n",
    "\\text{Residuals} = y - \\hat{y}(x) \\\\\n",
    "$$\n",
    "\n",
    "Of course, $y$ is the ground truth (true value given by our dataset), and $\\hat{y}(x)$ is the prediction of the strong learner.  \n",
    "These residuals are values roughly centered around 0 (depending on whether we have many outliers or not), like in the image above.  \n",
    "What's more important is that *these are the values we train our weak learners on*. Be it a shallow DT or any other kind of model, we try to fit it to the residuals.  \n",
    "\n",
    "The last step is to add the weak learner's prediction to the strong learner, multiplied by a coefficient. We have 2 ways to think about this coefficient:\n",
    "- as a learning rate, similar to how Gradient Descent works; this approach should be faster, since we don't need to find the best coefficient for each weak learner; at the same time, the final result might take longer to converge, so the final story is that it's a trade-off between speed and accuracy (as Gradient Descent teaches us).\n",
    "- as a way to control the \"importance\" of the weak learner; this approach works like AdaBoost, in that we try to find the best coefficient for each weak learner. This is slower, but it would allow us to find the best option at each step in the training process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, we have:\n",
    "$$\n",
    "\n",
    "\\hat{y}(x) = \\sum_{l=1}^L \\gamma_l \\cdot w_l(x) \\\\\n",
    "\\text{where $\\hat{y}(x)$ is the prediction of the strong learner} \\\\\n",
    "$$\n",
    "We find $w_l(x)$ by training a weak learner on the residuals:\n",
    "$$\n",
    "\\text{Residuals} = y - \\hat{y}(x) \\\\\n",
    "\\text{where y is the ground truth and} \\\\\n",
    "\\text{$\\hat{y}(x)$ is the prediction of the strong learner.}\n",
    "$$\n",
    "And each weak learner is trained to fit to the residuals, and then added to the strong learner with a coefficient $\\gamma_l$:\n",
    "$$\n",
    "\\hat{y_l}(x) = \\hat{y_{l-1}}(x) + \\gamma_l \\cdot w_l(x) \\\\\n",
    "\\text{or} \\\\\n",
    "\\hat{y_l}(x) = \\hat{y_{l-1}}(x) - \\gamma_l \\cdot w_l(x)\n",
    "$$\n",
    "\n",
    "### IMPORTANT NOTE\n",
    "\n",
    "Our goal is to reduce residuals over time; our models are supposed to subtract from the strong learner's prediction. Although the first formula above uses a $+$ sign, we actually want to perform *subtraction*. We can do this by setting $\\gamma_l$ to a negative value, or by using a $-$ sign, like in the second formula. Although it seems like a small detail, it's important to keep in mind, which is why I've left it at the end with a big sign of warning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Gradient Boosting\n",
    "\n",
    "We've discussed Gradient Boosting and compared it to AdaBoost, in terms of iteratively adding weak learner's predictions to the strong learner.  \n",
    "\n",
    "I'd now like to add one more point.\n",
    "Imagine that we had a perfectly-fitting weak learner, which would mean that we could fit the residuals perfectly (forget overfitting for a second). In that case, we would have the biggest impact on the strong learner, since we would be able to subtract the residuals perfectly.  \n",
    "I now present the following question: what if, instead of looking for a model to fit the residuals, we used the residuals themselves? After all, a model is a function, and a function is a mapping from one set to its corresponding values. Why would that be any different than training a model?  \n",
    "\n",
    "If your answer is \"what about gaps in the residuals?\", then you're right. In an ideal world, instead of using models, we would have perfect datasets with 100% coverage of the possibility space. In reality, we have gaps. We train models to fill those gaps to the best of our ability. We cannot hope to fill a continuous function with a discrete set of points, so we use models to approximate the function.  \n",
    "\n",
    "As a note of completion, in the ideal world where our data covers the entire possibility space, we might write the formula for the strong learner as follows:\n",
    "$$\n",
    "\\hat{y_l}(x) = \\hat{y_{l-1}}(x) - \\gamma_l \\cdot \\nabla_{\\hat{y_{l-1}}(x)} \\mathcal{L}(y, \\hat{y_{l-1}}(x)) \\\\\n",
    "\\text{Where $\\nabla_{\\hat{y_{l-1}}(x)} \\mathcal{L}(y, \\hat{y_{l-1}}(x))$ is the gradient of the loss function at the point $\\hat{y_{l-1}}(x)$}\n",
    "$$\n",
    "\n",
    "Again, since we are not in an ideal world, we use models to approximate the gradient, so we substitute the $\\nabla_{\\hat{y_{l-1}}(x)} \\mathcal{L}(y, \\hat{y_{l-1}}(x))$ with a model:\n",
    "\n",
    "$$\n",
    "\\hat{y_l}(x) = \\hat{y_{l-1}}(x) \\pm \\gamma_l \\cdot w_l(x)\n",
    "$$\n",
    "(again, depending on the sign of $\\gamma_l$)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion - Gradient Boosting\n",
    "\n",
    "Gradient Boosting is a method that uses weak learners to iteratively improve the strong learner. It does so by training weak learners on the residuals of the strong learner, and then adding their predictions to the strong learner with a coefficient, which can be interpreted as a learning rate, or as the best coefficient for each weak learner at each step in the training process.  \n",
    "\n",
    "As with AdaBoost, we iterate over multiple learning steps which involve weak learners, so that we might reach a more accurate strong learner. Since we're talking about a boosting method, the overall goal is to reduce bias (therefore increase variance), so we must pay attention to overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- [Towards Data Science](https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205)\n",
    "- [Wikipedia - AdaBoost](https://en.wikipedia.org/wiki/AdaBoost)\n",
    "- [Analytics Vidhya - AdaBoost](https://www.analyticsvidhya.com/blog/2021/09/adaboost-algorithm-a-complete-guide-for-beginners/)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commonMLalgs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13 (main, Oct 13 2022, 21:15:33) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "500cef33aa921c87c63c1753d003ac956bc931603b9c07a450ae237ef80e36a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
