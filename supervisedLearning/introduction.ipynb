{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep in mind I'm using notebooks to alternate between implementations and explanations / images where possible.\n",
    "\n",
    "Libraries we'll be using:\n",
    "- Numpy\n",
    "- Scipy\n",
    "- Matplotlib\n",
    "- Scikit-Learn\n",
    "\n",
    "If you have these, I'd say the basics are covered. I'll update if necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "\n",
    "This is an important one. Algorithms that fall within this class are the vast majority of everything that is in use today.\n",
    "\n",
    "### What is it?\n",
    "\n",
    "Let's split the name:\n",
    "- Supervised - something that is observed/overseen during its actions\n",
    "- Learning - the act or process of acquiring knowledge or skill  \n",
    "\n",
    "We know why we're here, so we know what the \"Learning\" part is. But what about the \"Supervised\" part?  \n",
    "Here is what I want you to know:\n",
    "### We know the output.  \n",
    "That's it, essentially. All algorithms denoted as \"supervised\" are those that we know the output of. This means our job is to find a function that maps the input to the output.  \n",
    "In other words, we have our data (input and output) and we want our algorithm to predict that same output. Then, we hope that new data gets a good enough output so we can use it in the future.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "We often see this term used, especially when it comes to supervised learning. The idea behind it is simple. We know what we want our output to be, so we measure how far the current prediction is. That's our **error**, or **cost**. We sum these errors up and we have our **cost function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common cost functions:\n",
    "- Mean Squared Error\n",
    "- Mean Absolute Error\n",
    "- Cross Entropy (this one is used in *classification problems*)\n",
    "\n",
    "**Formulas for these:**\n",
    "\n",
    "***\n",
    "\n",
    "<center>MSE:</center>\n",
    "\n",
    "$$\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$$\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<center>MAE:</center>\n",
    "\n",
    "$$\\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y_i}|$$\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<center>Cross Entropy:</center>\n",
    "\n",
    "$$-\\frac{1}{n}\\sum_{i=1}^{n}y_i\\log(\\hat{y_i}) + (1-y_i)\\log(1-\\hat{y_i})$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('commonMLalgs')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "500cef33aa921c87c63c1753d003ac956bc931603b9c07a450ae237ef80e36a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
